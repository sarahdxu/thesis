\documentclass[12pt]{article}
\usepackage{graphicx,booktabs}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{float}

\makeatletter
\renewenvironment{abstract}{%
    \if@twocolumn
      \section*{\abstractname}%
    \else %% <- here I've removed \small
      %
        {\bfseries \Large\abstractname\vspace{\z@}}%  %% <- here I've added \Large
      %
      \quotation
    \fi}
    {\if@twocolumn\else\endquotation\fi}
\makeatother


\graphicspath{ {images/} }
\usepackage[letterpaper, portrait, lmargin=1.5in, rmargin=1.25in, tmargin=1in, bmargin=1in]{geometry}
\usepackage{color}
\usepackage{setspace}
\usepackage[titletoc]{appendix}
\usepackage{pdfpages}
\usepackage{booktabs}
\usepackage[nottoc]{tocbibind}
\usepackage{caption}
\captionsetup[figure]{
    position=above,
}
\usepackage{dcolumn}
\usepackage[round]{natbib}
\bibliographystyle{plainnat}
\newcounter{mysubtable}
\usepackage{amsmath}
\usepackage{caption}
\newcommand\modcounter{%
  \refstepcounter{mysubtable}%
  \renewcommand{\thetable}{\thesection.\arabic{table}\alph{mysubtable}}%
}
\newcommand{\appendixnumberline}[1]{Appendix\space}

\let\oldappendix\appendix
\makeatletter
\renewcommand{\appendix}{%
  \addtocontents{toc}{\let\protect\numberline\protect\appendixnumberline}%
  \renewcommand{\@seccntformat}[1]{Appendix~\csname the##1\endcsname\quad}%
  \oldappendix
}
\makeatother

%\makeatletter
%%% The "\@seccntformat" command is an auxiliary command
%%% (see pp. 26f. of 'The LaTeX Companion,' 2nd. ed.)
%\def\@seccntformat#1{\@ifundefined{#1@cntformat}%
%   {\csname the#1\endcsname\quad}  % default
%   {\csname #1@cntformat\endcsname}% enable individual control
%}
%\let\oldappendix\appendix %% save current definition of \appendix
%\renewcommand\appendix{%
%    \oldappendix
%    \newcommand{\section@cntformat}{\appendixname~\thesection\quad}
%}
%\makeatother

    \usepackage{tabularx}
\usepackage{adjustbox}

%\usepackage[toc,page]{appendix}



\doublespacing


\title{The External Validity of Experimental Social Preference Games}
\author{Sarah Xu}
\date{April 2018}

\begin{document}

\maketitle
\newpage
\singlespacing
\tableofcontents


\newpage


\section*{Acknowledgements}
\addcontentsline{toc}{section}{Acknowledgements}
Acknowledgements to people


\newpage

\section*{Abstract}
\addcontentsline{toc}{section}{Abstract}

\doublespacing
The external validity of experimental games is a growing topic of interest. This topic is important because researchers commonly use such games to study individuals\rq \ social preferences. Many papers have conducted within-subjects experiments, comparing subjects\rq \ in-lab decisions to their real-world decisions. However, the current accumulated evidence yields mixed conclusions: some papers find statistically significant lab-field correlations, while other papers do not. In this research, I compared behavior in various social preference games and self-reported measures regarding past social behavior to a natural, far-removed field measure. My results show social preference games do a poor job explaining the field behavior. Furthermore, self-reported measures perform just as well, if not better, than the games in both explaining and predicting the field measure.



\newpage


\doublespacing
\section{Introduction}

The standard economic model assumes individuals\rq \ actions are motivated purely by self-interest. However, from simple observation it is clear that people care about the well-being of others: people volunteer, people donate to causes, and social welfare programs exist. If someone is not solely motivated by material self-interest but also cares about the well-being of others, we say that the person has social preferences. 

The last few decades have seen a strong increase of interest in this topic, with numerous research studies providing evidence of social preferences. When studying social preferences, a common approach is to conduct experiments in a laboratory setting. Participants play experimental games, where they receive monetary incentives aligned with the payoffs of the games. Researchers are able to control and influence prices, information, and actions available to the participants, which allows researchers to rigorously target different aspects of social behavior\footnote{Many papers have identified various aspects of social preferences such as altruism, social welfare, inequality aversion, and reciprocity (e.g., \cite{charness_rabin_2002}; \cite{fehr_schmidt_1999}; \cite{rabin_1993}; \cite{fisman_jakiela_kariv_2014}).}. Below are four common games used to study social preferences, which I used in this research. 

The dictator game involves a single player (the dictator) who receives an endowment. The player must choose how to split the amount between themselves and a second player (the receiver). 

The ultimatum game is a two-player game where the players bargain over a fixed endowment. The first mover (the proposer) divides the sum, and the second mover (the responder) chooses to either accept or reject the offer. If accepted, the proposal is implemented. If rejected, neither player receives any money. 

The trust game is also a two-player game. The proposer receives an endowment and proposes how to divide the endowment between themselves and the second mover. The offered amount is multiplied by a factor, and the responder decides how much of the multiplied endowment to return to the proposer. 

Finally, the public goods game is an N-player game where each player receives the same sum of money and simultaneously decides how much to put into a public fund. The total amount in the public fund is multiplied by a factor, and divided evenly amongst the players.

Since there are numerous factors that can lead to differences in behavior in a lab setting versus field setting, an important question is the external validity of these social preference games -- that is, whether the decisions made in the games can be generalized to decisions made in real-world situations. Many papers explore this question, comparing subjects\rq \ lab behavior  to the same subjects\rq \ real-world behavior. \cite{galizzi_navarro-martinez_2017} summarized that about 40\% of reported lab-field correlations and regressions found statistically significant associations between lab and field behaviors\footnote{See Galizzi and Navarro-Martinez for their full systematic review and meta-analysis of literature on within-subjects lab-field studies.}. Therefore there is room for further exploration. Similar to previous lab-field experiments, I examined whether in-lab social behavior predicts real-life social behavior. Adopting the experimental design in the study by Galizzi and Navarro-Martinez to provide a systematic approach to the question, I recruited Wesleyan University seniors and recent alumni to play various experimental games. I also had the participants answer self-reported questions regarding their past social behavior as an additional layer to evaluate the predictive ability of the games, and I used donations to Wesleyan University as the field measure.

My results show that social preference games explain little to none of the donations behavior. In fact, the self-reported measures seem to perform just as well as the games. In addition, self-reported measures seem to play a more important role when predicting the field measure.

\newpage

\section{Literature Review}

There are many papers that study social preferences using experimental games. For example, \cite{forsythe_1994} recruited undergraduate students from the University of Iowa, where they played single dictator and ultimatum games against anonymous opponents. The authors found that, contrary to the selfish subgame perfect Nash equilibrium in which both proposers and responders do not send any money, most players did share proportions of their endowments. \cite{berg_1995} recruited undergraduate students from the University of Minnesota to participate in trust games. They found that people showed high levels of trust and reciprocity: 94\% of the proposers sent money, and over one-third of the responders returned an amount greater than what was given to them. \cite{hermann_thoni_gachter_2008} conducted public goods experiments across 16 participant pools, and found that that varying opportunities for punishment led to varying cooperation levels. These papers are only a few examples from an abundance of literature, making it clear that using social preference games has become one of the building blocks of experimental and behavioral economics. 
 
However, human behavior may be influenced by a variety of factors. One issue is that the subject\rq s actions are under the scrutiny of the researcher. \cite{hoffman_1994} found that almost 50\% of their subjects donated at least \$3 (out of a \$10 pie) when playing the dictator game. However, when the authors implemented a ``double-blind'' treatment where both the experimenter and other subjects could not observe the dictator\rq s actions, they found that only 16\% of subjects gave at least \$3. There are other papers that also concluded that subjects are more prosocial in the lab than they are in the field (e.g., \cite{list_2006}; \cite{gneezy_2004}). Individuals are also influenced heavily on the framing of the situation. For example, in a two-person trust game, \cite{burnham_mccabe_smith_2000} switched between calling the responder as ``partner'' or ``opponent'', and found that trustworthiness with ``partner'' was over twice that for ``opponent''. Similarly, \cite{ross_ward_1996} found that participants showed high levels of cooperation when playing a prisoner\rq s dilemma game called a ``community'' game, and participants showed lower levels of cooperation when the game was called a ``Wall Street'' game. Some studies also found that varying the level of stakes may lead to significantly different behaviors. For example, \cite{carpenter_verhoogen_burks_2005} discovered that increasing the stakes from \$10 to \$100 decreased the median offer in the dictator game from 40\% to 20\%. \cite{slonim_roth_1998} also found that in the ultimatum game, rejections occurred less frequently and proposal amounts decreased as stakes increased. Interestingly, however, \cite{cherry_frykblom_shogren_2002} found no differences in offers when increasing the stakes from \$10 to \$40 in the dictator game. 
 
These examples demonstrate that such factors can yield significant deviations in game behavior. On a larger scale, since experiments conducted in lab setting are abstract and remote from realistic situations, there may also be many factors that vary between lab and field setting which can influence decision-making. For example, people know their actions are being recorded during an experiment, but when making real-life choices, their decisions are made in private. Pro-social decisions are also more likely to have more context and meaning in real life situations than when participating in experimental games. As such, actions made in a lab setting may not reflect the person\rq s real-world behavior\footnote{See \cite{levitt_list_2007} for a full discussion of factors, with supporting literature, that may lead to deviations between game behavior and real-world behavior.}. Therefore an important question is the extent to which behavior from experimental games can be extrapolated to behavior in the field.

Some studies have examined how subjects\rq \ behavior in a lab setting is compared to the same subjects\rq  \ behavior in a real world situation, and found that in-lab behavior does explain field behavior. \cite{baran_2010} compared MBA alumni donations to their university with their reciprocity behavior when playing the trust game. The authors found that responder behavior predicted university donations. \cite{franzen_pointner_2012} compared decisions from university students participating in standard dictator games to their actions when receiving a misdirected letter containing money, and found that subjects who showed prosocial behavior in the lab returned the misdirected letters more often than subjects who were selfish in the lab. \cite{englmaier_gebhardt_2011} conducted field experiments where they compared university students\rq \ free riding behavior at the library to free riding behavior in a public goods game, and found statistically significant correlation between field and lab measures. There are also studies that used non-student subjects: \cite{fehr_leibbrandt_2011} conducted public goods games with Brazilian fishermen, and found that those who were more cooperative in the games were also less likely to exploit the communal fishing grounds. \cite{karlan_2005} compared subjects\rq \ trust game behavior to their loans repayment behavior in a Peruvian microfinance program, and found that those who showed ``trustworthy'' behavior were less likely to default on their loans. 

On the other hand, a number of papers found lab behavior had no explanatory power for field behavior. \cite{goeschl_2015} examined university students\rq \ behavior in two different tasks: a public goods game and their contributions to a task about reducing CO$_{2}$ emissions. The authors found that decisions in both tasks were uncorrelated. \cite{hill_gurven_2004} carried out ultimatum and public goods games on Paraguay Ache Indians. They compared the game behaviors to observed food production and sharing patterns with individuals outside the nuclear family, and found no significant relationship to lab behavior. \cite{gurven_winking_2008} recruited Tsimane forager-horticulturalists in Bolivia, and compared behavior when playing dictator and ultimatum games to their food-sharing behavior. The authors concluded no relation between the two measures. \cite{voors_2012} used farmers in Sierra Leone, and compared their decisions in a public goods game to their decisions when asked to contribute to a real community public good. They found no meaningful correlation in behavior between the lab and field measures. 

With studies finding statistically significant, not statistically significant, and mixed results, it is clear that evidence for the external validity of experimental games is weak. Galizzi and Navarro-Martinez noted, however, that the previous studies compared only one social preference game to one specific field measure. The theoretical nature of the games makes it difficult to conceptually map them to a specific field measure, and so it is crucial to have a more systematic approach. The authors conducted their own study where participants answered questions about social behaviors exhibited in the past, played various social preference games, and encountered naturalistic field situations. The field situations included a research assistant asking for help carrying boxes down the stairs, asking to use the participant\rq s phone to make a brief phone call, asking for donations to a children\rq s charity, asking for donations to an environmental charity, or asking for donations to the lab\rq s research fund.  The authors\rq \ overarching conclusion was that behavior when playing the experimental games does a poor job predicting both the survey questions and the field behaviors. They mentioned, however, that more systematic studies are needed in order to draw a definite conclusion.

Similar to Galizzi and Navarro-Martinez, my research provides another systematic approach by having participants play multiple social preference games. Furthermore, it is critical to think about why some studies found correlations between lab and field measures and some studies did not, in order to further inform my experiment design. While playing the experimental games for the study by Hill and Gurven, participants expressed worry that their choices would make the receiver upset. In particular, the tribal group\rq s culture was heavily focused on community, and was well known for extensive food sharing (which was the field measure the authors used). Perhaps the participants didn\rq t want to risk their choices creating any tension in their community, and getting punished with less food sharing and cooperation. In the studies by Gurven and Winking and Voors et al., participants were also subject to high scrutiny and low anonymity since it was easy for community members to find out the choices each subject made. Therefore an important aspect in my research design is that participants played the games in their own time and location. This set-up ensured participants had minimal scrutiny and higher anonymity. 

In addition, a criticism of the experimental design by Galizzi and Navarro-Martinez is how the authors executed their field measures. Since the field situations occured as the participants were exiting their lab session, it is hard to believe that the participants did not connect the encounters to the research lab, especially since they previously answered questions similar in fashion. This may have led to actions that participants would not have done if they were unaware of the scrutiny. This is called the experimenter demand effect, which is a common problem in experimental design: if participants know their actions are being scrutinized, they may change their behavior towards what constitutes appropriate behavior. Therefore it is better to use a natural, far-removed situation, which is why I used donations to Wesleyan University as my field measure.

\section{Methods}

Each participant was presented with two sets of tasks: (i) incentivized social preference games, and (ii) self-reported questions regarding past social behavior. The field measure used to compare to participants\rq \ lab results is their donations to Wesleyan University.  In addition, I examined which tool, the social preference games, the self-reported measures, or both, is better at predicting social preferences exhibited in real-world situations.
 
Wesleyan University seniors and recent alumni received an invitation email briefly explaining my research and asking for participation in my study (2,004 emails were provided by Wesleyan University Relations). The entire study was computerized, programmed and implemented using Qualtrics. Online links to both the experimental games and survey questions were included in the email, so those who chose to take part could complete the study remotely. As mentioned in the previous section, the online structure allowed participants to make decisions anonymously and with minimal scrutiny, reducing the risk for exaggerated behavior. Participants received an overview of the two tasks they would complete, and were informed that upon completion of the study, their major, class year, and Wesleyan donations information would be released to me.

In order to incentivize completion of both the games and survey questions, as well as elicit honest game behavior, participants were informed that completing the entire study made them eligible for a lottery prize. All of the games used ``tickets" as the experimental currency unit, where the total tickets each participant earned in the games equaled how many lottery tickets they owned. Ten tickets were randomly drawn, and each winner received \$100.


\subsection{Incentivized social preference games}

Those who chose to participate in the study received a unique identifier assigned by Qualtrics. Participants first played four social preference games (in random order): the generalized dictator game, the ultimatum game, the trust game, and the public goods game. Before each game, they received detailed instructions along with an example to illustrate how each game worked. For the ultimatum and trust games, each participant had the opportunity to play the roles of both proposer and responder. At the end of the participation deadline, all participants were randomly paired, and ticket payoffs were calculated.

Below are descriptions of each game the participants played, along with examples (see Appendix C for screenshots of each game, along with respective instructions and examples, from the online online study).

\subsubsection{Generalized dictator game}

Each participant, playing as the dictator, played nine different rounds of the generalized dictator game\footnote{The design is based off of the study by \cite{andreoni_miller_2002}.}. In each round, participants were given different endowments and prices of giving, and were asked how they would like to divide the endowment with an anonymous, random Player 2. Therefore the budget constraint is given by \(\pi_{s} + p\pi_{o} = \textit{m}\), where \(\pi_{s}\) is how many tickets were kept, \(\pi_{o}\) is how many tickets were given, p is the relative price of giving, and \textit{m} is the endowment. The endowment in each round was either 10, 12, or 15. Every ticket the participant kept was worth either 1, 2, 3, or 4 tickets (hold price), and every ticket given to Player 2 was also worth either 1, 2, 3, or 4 tickets (pass price). Below are the nine sets of endowments and prices of giving used in the study:

\onehalfspacing
\begin{center}
\begin{tabular}{ c c c c c }
\hline \hline
 Budget & Ticket Endowment & Hold Price & Pass Price & Relative Price of Giving \\ 
 \hline
1 & 15 & 1 & 2 & \(\frac{1}{2}\)  \\  
2 & 10 & 1 & 3 & \(\frac{1}{3}\)  \\  
3 & 15 & 2 & 1 & 2 \\  
4 & 12 & 1 & 2 & \(\frac{1}{2}\)  \\  
5 & 10 & 3 & 1 & 3  \\  
6 & 15 & 1 & 1 & 1  \\  
7 & 12 & 2 & 1 & 2 \\  
8 & 10 & 4 & 1 & 4 \\  
9 & 10 & 1 & 4 & \(\frac{1}{4}\)  \\ 
\hline \hline \\
\end{tabular}
\end{center}

\doublespacing
Participants were given the endowment and hold/pass price for each round, and were presented an interactive slider bar that displayed the different ticket amounts each player could earn. The slider bar ensured choices fulfilled the budget constraint, and eliminated the need to perform calculations by directly showing how many tickets each player would receive.

\underline{\textit{Example:}} Divide 10 tickets: Hold @ 1 ticket each, Pass @ 3 tickets each. In this example, the endowment is 10 tickets, and the relative price of giving is \(\frac{1}{3}\). That is, however many tickets the player decides to keep is multiplied by 1 (the hold price), and however many tickets given to Player 2 is multiplied by 3 (the pass price). The player is presented a slider bar, ranging from giving 0 tickets to giving 10 tickets, and with each possible offer amount, the player was told the total ticket amounts received by both players in the form (total tickets received, total tickets given). For example, if the player decides to give 4 tickets to Player 2, they would see (6, 12) which means they would receive 6 tickets (10 - 4 = 6) and Player 2 would receive 12 tickets (4 * 3 = 12).

\subsubsection{Ultimatum game}

\underline{\textbf{Player 1:}} Each participant was endowed with 10 tickets, and were told to decide how much of their endowment to send to an anonymous, random responder (Player 2) so that \(\pi_{s} + \pi_{o} = 10\). They were also informed that the responder may or may not reject the proposed allocation: if the allocation is accepted, then the proposal is implemented (Player 1 will receive \(\pi_{s}\) and Player 2 will receive \(\pi_{o}\)), and if the allocation is rejected, neither player receives any tickets.

 \underline{\textit{Example:}} Suppose the participant chooses to give 3 tickets to Player 2. If Player 2 accepts the proposal, then the participant will receive 7 tickets (10 - 3 = 7) and Player 2 will receive 3 tickets. However, if Player 2 does not like the proposal and rejects the proposition, then both players will receive 0 tickets.
 
\underline{\textbf{Player 2:}} Each participant was informed that the proposer (Player 1) was given an endowment of 10 tickets. They were presented a list from 0 tickets to 10 tickets (in 1-ticket increments), which represented the different amounts that Player 1 could choose to give them. The participants were instructed to indicate whether they accept or reject each hypothetical proposal, and were told that accepting means they agree to receiving the offered amount, and rejecting means they do not agree and instead both players will receive 0 tickets.

\underline{\textit{Example:}} Suppose the participant chooses to reject all offered amounts below 5 tickets, and accept all offered amounts equal to or greater than 5 tickets. If Player 1 chose to offer 3 tickets, then both players will receive 0 tickets. However, if Player 1 chose to offer 6 tickets, then Player 1 will receive 4 tickets (10 - 6 = 4) and the participant will receive 6 tickets.

	
\subsubsection{Trust game}
\underline{\textbf{Player 1:}} Each participant was endowed with 10 tickets. They were prompted to decide how much of their endowment to send to an anonymous, random responder (Player 2) so that \(\pi_{s} + \pi_{o} = 10\). The participants were told that the amount sent over would be multiplied by 3, i.e. Player 2 would receive 3\(\pi_{o}\), and that Player 2 would then decide how many tickets, r \(\leq 3\pi_{o}\), they would like to return. Overall, Player 1 received \(\pi_{s}\)+r tickets and Player 2 received 3\(\pi_{o}\) - r tickets.

\underline{\textit{Example:}} If the participant chooses to send 5 tickets, then Player 2 will receive 15 tickets (5 * 3 = 15). Player 2 will then decide how many tickets to return. If Player 2 chooses to 3 tickets, then overall Player 1 will earn 8 tickets (5 + 3 = 8) and Player 2 will earn 12 tickets (15 - 3 = 12).

\underline{\textbf{Player 2:}} Each participant was informed that the proposer (Player 1) received an endowment of 10 tickets. They were then given a list of all ten possible multiplied amounts that Player 1 could have chosen to send, ranging from 3 tickets to 30 tickets in 3-ticket increments. For each possible offered amount, the participants were prompted to enter the number of tickets they would like to return back to the proposer. They were given clear instructions that their input could not exceed what was given to them. For example, if Player 1 gave them a multiplied amount of 15 tickets, then the maximum number of tickets the participant could return is 15 tickets.

\underline{\textit{Example:}} Suppose the participant\rq s return scheme is as below: \\
\onehalfspacing		
\begin{center}
\begin{tabular}{ c c }
\hline \hline
 Possible Offer Amounts (Multiplied) & Tickets Returned \\ 
 \hline
3 & 0  \\  
6 & 1 \\  
9 & 3  \\  
12 & 3  \\  
15 & 5   \\  
18 & 5  \\  
21 & 5 \\  
24 & 5 \\  
27 & 5 \\  
30 & 5 \\  
\hline \hline \\
\end{tabular}
\end{center} 
\doublespacing
where the first column is a list of all possible multiplied amounts that Player 1 could choose to send, and the second column indicates how many tickets the participant would like to return. If Player 1 decided to give 7 tickets to the participant, then the participant will receive a multiplied amount of 21 tickets (7 * 3 = 21). As indicated by the participant\rq s return scheme, the participant chose to return 5 tickets. Thus Player 1 receives 8 tickets (3 + 5 =  8), and the participant receives 16 tickets (21 - 5 = 16).

	
\subsubsection{Public goods game}

Each participant was given 10 tickets, and told that they would be randomly matched with one other player. They were then prompted to decide how much of their endowment to contribute to a group fund, and were informed that the other player was told to do the same task. The total tickets in the group fund was multiplied by 2, and divided evenly between the two players. Therefore each player received the remaining amount of their endowment plus the divided amount from the public fund. The payoff function is given by \( P_{i} = 10 - g_{i} + \frac{2\sum_{n=1}^{2} g_{n}}{2} \) where \(g_{i}\) is the amount that player \textit{i} donated to the fund, and \(\sum_{n=1}^{2}g_{n}\) is the sum of both players\rq \ donations to the public fund.

\underline{\textit{Example:}} Suppose the participant chooses to contribute 3 tickets, and the second player chooses to contribute 5 tickets. The total donated amount, \(\sum_{n=1}^{N} g_{n}\), is 8 tickets. Therefore the participant receives: 

\( P_{1} = 10 - g_{1} +  \frac{2\sum_{n=1}^{2} g_{n}}{2} = 10 - 3 + 8  = \) 15 tickets 

\noindent and the second player receives: 

\( P_{2} = 10 - g_{2} +  \frac{2\sum_{n=1}^{2} g_{n}}{2} = 10 - 5 + 8 = \) 13 tickets 


\subsubsection{Strategy Method}

Since the participants were completing the games at their own availability, they were randomly matched with other participants after all data had been collected. In the two-player games (ultimatum and trust games) which required both a proposer and responder, each participant played as both roles. When playing as the proposer, the participant decided how to split the endowment. When playing as the responder, the participant saw a list of all possible offer amounts and was prompted for their choice for each offer. This is called the strategy method.

There are several reasons why the strategy method is advantageous. First, it is useful to have all of the responder\rq s potential choices so that payoffs can be determined after the fact. Most importantly, the strategy method gives more information. If players were matched with other players live, then we would only have the responder\rq s response given the proposer\rq s offer. The strategy method, however, provides all returned amounts for all possible donated amounts in the trust game, and provides participants\rq \ minimum accepted amount in the ultimatum game. 

\subsubsection{Payment}

Participants were randomly paired, and ticket payoffs were determined for each game. In each assigned pair, one participant was randomly selected to be Player 1 (i.e. they were the dictator in the generalized dictator game, and proposer in both the ultimatum game and trust game), and the other was Player 2 (i.e. they were the receiver in the generalized dictator game, and the responder in both the ultimatum and trust game). I ran through each game and calculated ticket payoffs for each participant. Since the generalized dictator game consisted of nine rounds, one round was randomly selected; each participant received the tickets based on their decision for that round. 

After each participants\rq \ total lottery tickets were calculated, ten tickets were drawn. Each ticket owner was emailed instructions on how to receive their \$100 prize through direct deposit. 

\subsection{Self-reported measures of past social behaviors}

Participants then reported on past pro-social behavior. The questions were adapted from the Self-Report Altruism (SRA) scale introduced by \cite{rushton_chrisjohn_fekken_1981}. The survey was comprised of 10 items, and participants reported how frequently they have done each item. Participants rated each statement as either ``Never'', ``Once'', ``More than once'', ``Often'', or ``Very often''. Examples included ``I have donated money at the cash register when buying groceries'' and ``I have pointed out a clerk\rq s error (at the supermarket, at a restaurant) in undercharging me.'' A full list of the items from the online study is displayed in Appendix D. 

\subsection{Field Measure}

After the participation deadline passed, Wesleyan University Relations provided a dataset that contained each participant\rq s Wesleyan donations information.

There are several reasons why I chose to use donations as my field measure. First, many behavioral economics papers that explored the relationship between in-lab and field behavior used donations\footnote{See, e.g., \cite{benz_meier_2006}; Baran et al.; Falk et al.}. \cite{falk_2013} justified that donations are an accurate field measure because they do not rely on self-reported responses but on actual decisions. Furthermore, donations are made in private and never made public, and students/alumni are unaware that their actions will be analyzed in a future research study. The lack of scrutiny therefore indicates donations should reflect the donors\rq \ genuine altruism. The authors then pointed out the most important reason for their decision to use donations: all students and alumni are solicited, so everybody has to make the decision about donating.

In addition, donations are a more accurate field measure than creating a scenario at the end of the online session. I previously mentioned that in the experiment by Galizzi and Navarro-Martinez, participants were subject to the experimenter demand effect: because participants encountered the field experiments shortly after completing the lab experiments, it would not be difficult for participants to link the two situations together. Therefore, participants could have acted more pro-socially than they would usually under normal circumstances. At Wesleyan University, seniors are solicited around once per semester. Alumni are typically solicited for donations at the end of November / beginning of December, March, and June. Since the online study in this research was released between solicitation cycles for both seniors and recent alumni (early January 2018), and donations information was provided as of January 2018, participants\rq \ donations behavior are a far-removed situation and thus are not influenced by the experimenter demand effect.


\subsubsection{Theoretical Correspondences Between Games and Donations}

All of the social preference games I chose for my study tap into different types of pro-social behaviors that are also related to donating. First, the dictators in the generalized dictator game, proposers in the ultimatum and trust games, responders in the trust game, and players in the public goods game have the option to keep their endowment to themselves, but may also choose to share their endowment with another person. Therefore the actions in these games can be explained by altruism: those who choose to keep their endowment exhibit lower levels of altruism, and those who choose to share their endowment demonstrate higher levels of altruism. Likewise, altruism is the most common representation for donating since the donor chooses to give their income to the university to benefit future students. 

Proposers\rq \ behaviors in the trust and public goods games indicate their trust levels. In the trust game, since the proposers\rq \ offerings get multiplied by a factor, the proposers may exhibit trust by sharing their endowment with the hopes of the responders sending back an amount larger than what was given. Similarly, in the public goods game, the total amount donated into the public fund gets multiplied by a factor. Thus, players who donate some of their endowment into the public fund may do so with the hopes that the other players will do the same, and they will receive a net amount larger than their initial endowment. Correspondingly, donating may also represent trust since donors are trusting that the University will use their donations to benefit future students.

Responders\rq \ actions in the trust game can also be interpreted by reciprocity: if the proposers decided to share their endowment, the responders then has the opportunity to give back to the proposers. The responders display higher levels of reciprocity if they return some of what was first given to them, and display lower levels of reciprocity if they give back little or nothing. The responders\rq \ actions can also be represented by trustworthiness: the proposers may contribute some of their endowment in hopes of the responders returning a larger amount than what was offered. Thus responders who return a larger amount show higher levels of trustworthiness. Equivalently, donations may explain reciprocity and trustworthiness since seniors and alumni may donate as thanks for providing them invaluable resources and opportunities.

Players\rq \ responses in all of the games can also correspond with inequality aversion. Dictators in the generalized dictator game, proposers in the ultimatum and trust games, responders in the trust game, and players in the public goods game may choose to share their endowment so that the other player will not be left with nothing. Similarly, responders in the ultimatum game may exhibit fairness preferences by rejecting offer amounts of zero (and those who exhibit extreme fairness preferences may reject offer amounts lower than half the endowment). Donations can also symbolize inequality aversion: people may donate because they believe their donations will help students receive opportunities they would not be able to obtain otherwise. In addition, donors are able to target what area their donations can go towards: for example, if the donor targets their donations to Financial Aid, then they are displaying inequality averse preferences.

Lastly, players\rq \ behaviors in the public goods game can also be explained by their cooperation levels. Players may choose to contribute a part of their endowment because they believe the other players are doing the same thing. In the case for donations, people may also exhibit cooperation by donating because the University solicits for donations (i.e. they are cooperating with what the University is asking for), or donating because they believe other seniors and/or alumni are also donating.

	
\subsection{Participants and Sessions}

Wesleyan University Relations provided a random sample of 2,004 emails (334 emails for each class year from 2013 to 2018), and I sent an invitation email in early January 2018 asking for participation in my study. Participants were informed that the study consisted of several experimental games and non-incentivized survey questions. The participants were volunteers who opened the Qualtrics link, provided consent for me to receive their major, class year, and Wesleyan donations data, and completed both the survey questions and social preference games. The deadline for participation was mid-February 2018, and a total of 397 people completed the online study. Shortly after the participation deadline, all participants were randomly paired, ticket payoffs were calculated, and 10 winners were randomly selected. The winners were emailed  in the beginning of March 2018 with instructions on how to receive their prizes.

\section{Results}
The results are presented in four distinct sections. I first start by briefly describing the results obtained in the three main elements (social preference games, self-report measure of past social behaviors, and donations). The fourth section will focus on the main research question of the paper: the extent to which the games explain the field behavior. Appendix B contains the variables definition table, and Appendix A contains all of the figures, correlation tables, and regression tables from this section.

\subsection{Social Preference Games}
Since the generalized dictator game consisted of 9 rounds with varying sets of endowments and prices of giving, similar to Andreoni and Miller and \cite{fisman_kariv_markovits_2007}, I assumed each participant\rq s giving preferences was a member of the constant elasticity of substitution (CES) utility function.  The CES utility function is written as:\\


\(U_{s} = [\alpha(\pi_{s})^{\rho} + (1-\alpha)(\pi_{o})^{\rho}]^{1/\rho}\) \\


The first parameter, \(\alpha\), measures the relative weight on the payoff for self. Holding \(\rho\) constant, as \(\alpha \rightarrow 1\), \(U_{s} \rightarrow \pi_{s}\), i.e. the individual\rq s utility depends only on the amount they keep for themselves. As \(\alpha \rightarrow 0\), \(U_{s} \rightarrow \pi_{o}\), i.e. the individual\rq s utility depends only on the amount the other person receives. Therefore as \(\alpha\) approaches 1, the individual exhibits selfish preferences, and as \(\alpha\) approaches 0, the individual exhibits selfless preferences.

The second parameter, \(\rho\), indicates the willingness to trade off payoffs to themselves and the other person in response to price changes. Holding \(\alpha\) constant, \(U_{s} \rightarrow \alpha\pi_{s} + (1-\alpha)\pi_{o}\) as \(\rho \rightarrow 1\),  i.e. the individual\rq s utility depends on the sum of the players\rq \ payoffs. This means the individual exhibits perfect substitutes preferences for giving: they will prefer to give their entire endowment to the other player when the price of giving is cheap (p $<$ 1), and they will prefer to keep their entire endowment when the price of giving is expensive (p $>$ 1). These preferences are also called efficiency-minded preferences. As \(\rho \rightarrow -\infty\), \(U_{s} = \min(\alpha\pi_{s}, (1-\alpha)\pi_{o})\), that is, the individual\rq s utility equals the minimum payoff between both places, meaning the individual exhibits inequality-averse preferences. These preferences are called Leontief preferences: the individual prefers to split the endowment equally. Lastly, as \(\rho \rightarrow 0\), \(U_{s} \rightarrow A\pi_{s}^{\alpha}\pi_{o}^{1-\alpha}\). In this case, the individual has Cobb-Douglas preferences.\footnote{See \cite{arrow_1961} on how Leontief preferences and Cobb-Douglas preferences are derived from the CES production function.}

Maximizing utility subject to the budget constraint ( \(p_{s}\pi_{s} + p_{o}\pi_{o}=m\) ) yields the CES demand function given by: \\
 

\(\pi_{s}(p,m)=\frac{[\alpha/(1-\alpha)]^{1/(1-\rho)}}{\rho^{-\rho/(\rho-1)}+[\alpha/(1-\alpha)]^{(1/(1-\rho)}}m\) \\

\hspace{14.5mm} \(= \frac{A}{p^{r}+A}m\) \\
 
 \noindent
where \(r=-\rho / (1-\rho) \) and \(A=[\alpha / (1-\alpha)]^{1/(1-\rho)} \). This generates the following individual-level econometric specification for each participant \textit{i}: \\
 
\( \pi^{t}_{s,i} = \frac{A_{i}}{(p^{t}_{i})^{r_{i}} + A_{i}}m^{t}_{i} + \epsilon^{t}_{i}\) \\
 
\noindent
where \textit{i} represents each participant, \textit{t} represents each independent decision-problems in the generalized dictator game, and \( \epsilon^{t}_{i} \) is assumed to be distributed normally with mean zero and variance \(\sigma^{2}_{i}\). For each participant \textit{i}, I used the 9 combinations of \(\pi_{s}\), p, and \textit{m} and generated the estimates  \( \hat{A}_{i} \) and \( \hat{r}_{i} \) using non-linear least squares. From the estimates I retrieved each participant\rq s \( \hat{\rho_{i}}\), which then gave me each participant\rq s \( \hat{\alpha_{i}} \). I used \(\hat{\alpha}\) as the generalized dictator game parameter to indicate participants\rq \ selfishness, and \(\hat{\rho}\) as the second parameter to represent participants\rq \ efficiency levels.

Results for the proposers in the ultimatum game are also represented by the proposers\rq s pass rate, with higher pass rates indicating higher altruism and/or fairness preferences. For example, if the proposer sent 6 tickets to the responder, the pass rate is 0.6. Results for the proposers in the trust game are also represented by the proposer\rq s pass rate (the percentage of the initial endowment passed to the other responder). Higher pass rates indicate higher levels of altruism, fairness preferences, and/or trust. Similarly, outcomes for the players in the public goods games are represented by the proposer\rq s pass rate into the public fund. Thus the larger the amount the player contributes into the public fund signals the higher levels of cooperation the player has. 

Results from the responders in the ultimatum game are represented as the minimum pass rate the responder chooses to accept. Since responders were presented an ascending list of all possible amounts the proposer could choose to send, minimum pass rates were obtained using the switch point where the responders changed from rejecting an offer amount to accepting an offer amount. Lower minimum accepted pass rates indicate lower levels of selfishness and negative reciprocity, whereas higher minimum accepted pass rates indicate higher levels of selfishness and negative reciprocity.

The trust game asks for the responder\rq s return amount given the 10 possible offer amounts from the proposer. For each responder, I regressed their return amount on the offer amount, and retrieved the estimated slope. Thus the results from the responders in the trust game are represented by the estimated slope, which measures the participant\rq s reciprocity, or trustworthiness level. Values closer to one represent more reciprocal behavior, and lower levels closer to zero represent more selfish behavior.\footnote{There may be concern that some participant\rq s may not follow a linear trend. After plotting each participant\rq s return amount in response to the offer amount, I found that with the exception of a few participants, each participant\rq s data points follow a linear slope.}

Figure 1 consists of two panels (Panels A and B) that shows the distribution of parameters, \(\hat{\alpha}\) and \(\hat{\rho}\), derived from responses in the generalized dictator game.  The parameter estimates vary dramatically across subjects, implying that preferences for giving are very heterogeneous. Panel A displays a high peak of 21\% at \(\hat{\alpha}\)=1: a considerable amount of participants displayed extremely selfish preferences. There is a smaller peak of 12\% at \(\hat{\alpha}\)=0.5, and there are more \(\hat{\alpha}\) observations above 0.5 than below 0.5. These results show that participants tended to have more selfish preferences. 

To facilitate presentation of Panel B, participants with very negative \(\hat{\rho}\) values were combined into the leftmost bar. About 7\% of subjects had perfect substitutes preferences for giving (\(\hat{\rho} \approx 1\)): these subjects preferred to give their entire endowment to Player 2 when the price of giving was less than one, and preferred to keep their entire endowment when the price of giving was greater than one, i.e. efficiency-minded preferences. A little over 20\% of participants demonstrated Leontief preferences (\(\hat{\rho}\) far below 0), i.e. they preferred splitting the endowment equally, and roughly 15\% of subjects possessed Cobb-Douglas preferences (\(\hat{\rho} \approx 0\)). Many subjects also had intermediate values of \(\hat{\rho}\): 24\% had preferences for increasing total payoffs (\(0.1 \leq \hat{\rho} \leq 0.9\)), and almost 20\% had preferences for reducing differences in payoffs between self and others (\(-0.9 \leq \hat{\rho} \leq 0.9\)).

Figure 2 consists of four panels (Panels C, D, E, and F) which shows the distribution of responses in the ultimatum, trust, and public goods games. Panel C reports that 42\% of proposers in the ultimatum game gave half of their endowment to the responders, and there is slightly more emphasis on those who gave contributions lower than half the endowment than contributions higher than half the endowment. These results are in line with the typical patterns in previous literature, which finds that a majority of offers are in the range of 0.25-0.50. Correspondingly, Panel D shows the distribution of the minimum offers that the responder chose to accept. With the exception of one participant who showed extreme negative reciprocity by only accepting an offer of Player 1's entire endowment, everyone accepted an amount less than or equal to half of the endowment. 27\% of participants accepted an offer amount of 0 tickets. 

Panel E shows the distribution of contributions the proposers made in the trust game. Pass rates were scattered all across the range from offering none of their endowment to offering all of their endowment. There were two maxima, both around 22\%, at giving half the endowment or the entire endowment. Other contributions that have more than 10\% are proposers who gave 0.3 or 0.4 of the given endowment. This is also broadly in line with typical findings that report average transfers of roughly half of the endowment. Panel F shows that almost half the participants in the public goods game sent their entire endowment into the public fund. The next most popular choice (20\%) was to send half their endowment to the public pool, and only 4\% of participants sent nothing into the public pool. Again, this broadly matches usual results in literature.

Figure 3 contains two panels (Panels G and H) that displays the distribution of reciprocity levels and average repayment rate, respectively, which were derived from responders\rq \ responses in the trust game. Panel G shows that, with the exception of two participants who demonstrated negative reciprocity (perhaps they did not understand the game), reciprocity levels were very heterogenous between 0 and 1. There is a maximum (27\%) at reciprocity levels around 0.5, and a local maximum (14\%) around 0.35. There is also a small local maximum (10\%) at 0, which represents those who did not show any reciprocity. Panel H shows a strong peak of repayment rates of around 0.45, and participants showed slightly stronger preference repaying less than half of what was received versus repaying more than half of what was received. These results are in line with typical patterns found in previous literature, which report average repayment rates of nearly half of the transfer.

Table 1 shows the pairwise correlations (Spearman\rq s \(\rho\)) between the different game outcomes. A majority of the correlations are statistically significant at the 5\% level (13 out of 21). All of the statistically significant negative correlations involve \(\alpha\) from the generalized dictator game and responders\rq \ behavior in the ultimatum game. The negative correlations for \(\alpha\) reflect that participants who were more selfish (\(\alpha \rightarrow 1\)) were more likely to make smaller contributions in the other games, and the negative correlations for the responders in the ultimatum game reflect that participants who accepted smaller contributions were more likely to make larger contributions in the other game decisions. Otherwise, decisions made in the other games have positive correlations with one another. Overall, these results show that participants demonstrated consistent behavior in all the games.

\subsection{Self-Reported Measures of Past Social Behaviors}

Total SRA scores were obtained by summing across each participant\rq s responses for the 10 items in the SRA Scale (``Never'' = 0, ``Once'' = 1, ``More than once'' = 2, ``Often'' = 3, ``Very often'' = 4). A higher SRA score indicates higher pro-social behavior. Figure 4 shows the distribution of total scores. There is a wide variety in the total SRA scores obtained, ranging between 20 and 48. Scores are centered around 33 and the shape is symmetric. Panel J displays the distribution of monetary SRA scores, which were obtained by summing across each participant\rq s responses for only the items related to money (items 2, 3, 4, and 7 in Appendix B).  A large majority of the scores are between 9 and 12.

Table 2 contains pairwise correlations (Spearman\rq s \(\rho\)) between the game responses and the SRA scores. I included both total SRA scores (``SRA'') and monetary SRA scores (``SRAmoney''). None of the game results are significantly correlated with total SRA scores, and only the results from the responders in the trust game and players in the public goods game are significantly correlated with the monetary SRA scores. Overall, there is a very weak relationship between the social preference games and self-reported measures. 

\subsection{Donations Behavior}

Figure 5 shows the distribution of Wesleyan donations. Panel I shows that 64\% of the participants had donated to Wesleyan University before, and 36\% had not. Panel J shows the distribution of total donation amounts below \$100, with donations above \$100 aggregated into the rightmost bar. Most of the donation amounts are below \$20 (about 36\%). There are 30 participants who each donated a total amount greater than \$100. All of these larger donations amounts are between \$100 and \$625, with the exception of two extremely large donations.

Figure 6 shows that each class year has different donations behavior. Panel M shows that only a quarter of the senior participants had ever donated. In contrast, at least 60\% in each alumni class year had donated before. Panel N shows that each class year consecutively has higher average total donations, from seniors\rq \ average total donation of \$2 to the class of 2013\rq s average total donation of \$69\footnote{Outliers were dropped when calculating averages.}.

\subsection{External Validity of Social Preference Games}

\subsubsection{Do the Games Explain Donations Behavior?}
I now turn to the question of whether the game decisions explain the field behavior. There are two ways that I represented donations behavior: (1) whether the participant has ever donated or not, and (2) the total amount the participant has donated. I included the \(\alpha\) and \(\rho\) parameters from the generalized dictator game, proposers\rq \ pass rates in the ultimatum and trust games (``ultimatum1'' and ``trust1''), responders\rq \ minimum accepted pass rates from the ultimatum game (``ultimatum2''), responders\rq \ reciprocity levels from the trust game (``trust2''), and players\rq \ pass rates in the public goods game (``cooperation'') as the explanatory game variables. I also included total and monetary SRA scores (``SRAtotal'' and ``SRAmoney'') as the explanatory self-reported variables. In addition, I included class year dummy variables. Appendix B contains the table of variable definitions. 

Table 3 presents twelve logistic regression models using the binary donations representation as the response variable. The first seven columns present each of the game outcomes on their own, and the eighth column includes all of the game variables. The ninth and tenth columns contain SRAtotal and SRAmoney scores on their own, and the last two columns include all of the game outcomes along with the total and monetary SRA scores, respectively. All of the class year dummy variables are statistically significant at the 1\% level, and show that donation participation increases in class year. None of the games or SRA results are statistically significant on their own. When both the games and SRA results are used in the regression model, still none of the variables are statistically significant. 

%I then regressed the same explanatory variables on the amount donated. As shown in Figure 5, most donations amounts were fairly small. I therefore used two-limit tobit maximum likelihood, setting the restriction that 0 $\leq$ donations $\leq$ 50.  Table 4 presents the twelve tobit models, presented in the same format as Table 3. The total SRA score is statistically significant at the 10\% level and the monetary SRA score is statistically significant at the 5\% level. Otherwise, none of the social preference games results are statistically significant.
I then used ordinary least squares (OLS) to regress the same explanatory variables on the log of the amount donated. Table 4 presents the twelve linear regression models, presented in the same format as Table 3. Most of the class year variables are statistically significant at the 5\% level, and generally show that donation amounts increase in class year. Similar to the logistic regression models, none of the game outcomes or SRA scores are statistically significant.

%It is also useful to look at the R$^{2}$ statistic, which measures the proportions of variance explained by the regressors. For both the logistic and tobit regressions, I calculated each model\rq s McFadden\rq s pseudo-R$^{2}$ values\footnote{
%\(R^{2}_{McFadden} = 1 - \frac{log(L_{c})}{log(L_{null})}\)
%where \(L_{c}\) denotes the maximized likelihood for the current fitted model, and \(L_{null}\) denotes the maximized likelihood the model with no predictors.
%}. The values are shown at the bottom of Table 3. The highest value belongs to the model that includes both the games and monetary SRA score. Interestingly, the pseudo-R$^{2}$ value for the model with all of the game variables is lower than the pseudo-R$^{2}$ values for the models that include only the SRA scores. The pseudo-R$^{2}$ values for the tobit regression models are displayed at the bottom of Table 4. The values are very low, and again, the highest value belongs to the model that includes both the games and monetary SRA score. The pseudo-R$^{2}$ value for the model with all of the game variables is higher than the pseudo-R$^{2}$ value for the model that includes only the total SRA score, but is close to the pseudo-R$^{2}$ value for the model that includes only the monetary SRA score. Altogether, these results tell us that self-reported measures are a better tool than even using multiple social preference games when explaining whether people donate or not, multiple social preference games and money-related self-reported measures perform similarly when explaining how much people donate, but the best tool overall is to use both social preference games and self-reported measures. However, all of the R$^{2}$ values are very low - practically zero - so even the ``best'' model explains little to none of the variability of donations behavior.

It is also useful to look at the R$^{2}$ statistic, which measures the proportion of variance explained by the regressors. For the logistic regressions, I calculated each model\rq s McFadden\rq s pseudo-R$^{2}$ values which are displayed at the bottom of the table\footnote{
\(R^{2}_{McFadden} = 1 - \frac{log(L_{c})}{log(L_{null})}\)
where \(L_{c}\) denotes the maximized likelihood for the current fitted model, and \(L_{null}\) denotes the maximized likelihood the model with no predictors.
}. The highest value belongs to the model that includes all of the game outcomes and total SRA score. Interestingly, each model\rq s pseudo-R$^{2}$ values are similar, whether the model includes all of the game variables, only SRA scores, or a combination of the two. The R$^{2}$ values for the linear regression models are displayed at the bottom of Table 4. Again, the highest value belongs to the model that includes both the games and total SRA score, and each model\rq s R$^{2}$ values are similar. 

Altogether, these results tell us that self-reported measures perform similarly to multiple social preference games in explaining the field behavior. The best tool overall, though, is to use both social preference games and self-reported measures. Nevertheless, all of the models\rq \ R$^{2}$ values are low so even the ``best'' model explains only a little of the variability of donations behavior.

Tables 3 and 4 therefore provide evidence that the social preference games, even when combined with self-reported measures, have little power in explaining donations behavior. However, this may be because the right combination of variables are not included in the models. I performed best subset selection on both the logistic and linear regression models\footnote{Best subset selection identifies all of the possible regression models derived from all of the possible combinations of the candidate predictors, and determines the model that does the best at meeting some well-defined criteria. In this case, the criteria I used is Mallows\rq \ \(C_{p}\)-statistic, which assesses assess fits when models with different numbers of parameters are being compared.  See \cite{mallows_1973}.}. Since best subset selection fits all possible models to find the best candidate, I included more explanatory variables: I added the responders\rq \ average return outcome from the trust game (``average return''), and replaced SRAtotal and SRAmoney with each of the ten individual SRA items (``SRA1'' - ``SRA10''). I first performed best subset selection for the logistic model, where Table 5 displays the five best subset logistic regression models along with the class year dummy variables. Notably four SRA items (items 3, 4, 8, and 10) appear in all five of the models, and items 3 and 4 are statistically significant at the 5\% level. Each model\rq s pseudo-R$^{2}$ values are larger than the previous logistic models in Table 3, with the highest value belonging to the model that includes \(\alpha\) and five SRA items (items 3, 4, 7, 8, and 10). However, the R$^{2}$ value is still low.

Table 6 reports the five best subset linear regression models with the class year dummy variables. Two game variables (ultimatum1 and trust2) as well as four SRA items (items 3, 4, 5, and 6) appear in all of the models. Ultimatum1 and trust2 are both statistically significant at the 5\% level, SRA items 3 and 6 are statistically significant at the 10\% level, and SRA items 4 and 5 are statistically significant at the 1\% level. All of the the R$^{2}$ statistics are larger than the values in Table 4. The highest R$^{2}$ value belongs to the model that contains the mentioned variables along with cooperation and trust1, but the model still explains only 32\% of the variability in donations behavior.

Overall, self-reported measures seem to perform just as well as social preference games in explaining the field measure. This is seen in Tables 3 and 4 where the models that included only the total/monetary SRA scores had similar R$^2$ values compared to the models that included all of the game parameters, and Tables 5 and 6 where the same SRA items consistently appeared in each of the best subset models. However, each of the estimated models\rq \ still explain only a little of the variability of donations behavior. Moreover, some of the game variables in the models as seen in Tables 3-6 have signs that are not expected. First, it makes sense that the coefficient on \(\alpha\) should be negative since higher levels of alpha indicate more selfish behavior. Ultimatum1, trust1, and cooperation should have positive relationships with donations since higher pass rates signal more pro-social preferences. Ultimatum2 is expected to have a negative relationship with donations since higher minimum accepted pass rates signal higher negative reciprocity, and trust2 should have a positive relationship with donations. However, in the logistic models, \(\alpha\), ultimatum1, ultimatum2, and cooperation have opposite signs than expected, and in the linear models, \(\alpha\), trust1, and trust2 have opposite signs than expected. Therefore both the extremely low R$^{2}$ values and unexpected direction of game coefficients suggest that the external validity of social preference games is poor.

\subsubsection{Do the Games Predict Donations Behavior?}

The poor results from the previous section lead to another interesting topic: perhaps the social preference games and self-reported measures can still be a useful tool to predict donations behavior. That is, if Wesleyan University wanted to predict donations from incoming alumni, they can possibly use social preference games and/or self-reported measures from older alumni to help anticipate donations behavior.

In predictive modeling, a common approach is to split the data into a training set and testing set. The training set is used to build and train the model, and once the model is ready, the model is tested on the testing set to determine its accuracy and performance. I used the data from the older alumni participants (those who graduated in 2013, 2014, 2015, and 2016) to create the training set, and used the data from the most recent alumni (those who graduated in 2017) to create the testing set\footnote{I did not use seniors as the testing set because they have very different donations behavior, as seen in Table 6. Using the most recent alumni class as the testing set is satisfactory since seniors will become incoming alumni. Moreover, it is more interesting to predict their donations behavior since Wesleyan University focuses more on alumni donations than senior donations and, as seen in Table 6, there is a significant jump in donations activity.}. 

Similar to the previous section, I estimated models for both the binary and continuous representations of donations behavior. The models I estimated included: the logistic/linear regression models, the best subset logistic/linear regression models, and the least absolute shrinkage and selection operator (lasso) models\footnote{The lasso is a useful regression analysis method that performs both variable selection and regularization in order to enhance prediction accuracy. The lasso estimate is defined by\\ \\
\( \beta^{lasso}=\mathop\mathrm{argmin}\limits_{\beta} \{ \frac{1}{2} \sum_{i=1}^{N} (y_{i} - \beta_{0} - \sum_{j=1}^{p}x_{ij}\beta _{j})^{2}+\lambda\sum_{j=1}^{p}|\beta _{j}|\} \) \\ \\
where \(\lambda\) is a free parameter set to minimize the out of sample error. Certain coefficients are truncated to be set to zero to effectively choose a simpler model that does not include those coefficients. In other words, lasso regression works like a feature selector that picks out the most important coefficients, i.e. those that are most predictive (and have the lowest p-values). See \cite{tibshirani_1996} for a full introduction on the lasso method.}. To compare how well the models performed, I tested each of the models on the testing set and looked at the mean-squared prediction error (MSPE)\footnote{\(MSPE = E[(g(x_{i}) - \hat{g}(x_{i}))^{2}]\), i.e. the expected value of the squared difference between the fitted values implied by the predictive model \(\hat{g}\) and the values of the (unobservable) model g}, where smaller MSPE values indicate better performing predictive models.

First I estimated logistic regression models on the training set. Table 7 reports the models. Each of the estimated models were then tested on the testing set, and the bottom row reports the corresponding MSPE values. The lowest MSPE belongs to the model that includes only the monetary SRA score. Next I performed best subset selection. Table 8 reports the best five models that were chosen, and again, the MSPE values are displayed at the bottom. The model that includes \(\alpha\), ultimatum2, and a few of the SRA items (items 3, 4, and 10) has the lowest MSPE. Lastly I used the lasso method, and all of the variables except for \(\alpha\), ultimatum2, and two of the SRA items (items 3 and 4) were shrunk to zero. Table 9 shows the lasso penalized logistic model\footnote{Statistical significance tests were based off of the proposed test by \cite{lockhart_2014}. See \cite{kyung_2010} and Tibshirani for a full discussion on standard errors for lasso predictions.} and its corresponding MSPE value.

I repeated the same steps for the linear models. Table 10 reports the linear regression models, where again the lowest MSPE corresponds to the model that includes only the monetary SRA score. I ran best subset selection, and Table 11 reports the top five models. The lowest MSPE belongs to the model that contains \(\rho\), trust2, ultimatum1, and four of the SRA items (items 1, 4, 5, and 6) . Finally, the lasso method truncated most of the variables to zero, leaving \(\rho\), average return, cooperation, and three SRA items (items 4, 5, and 6). Table 12 shows the lasso penalized linear model and its MSPE value.

Below are tables that report each predictive model\rq s MSPE values:
\onehalfspacing
\begin{center}
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{ c | c | c }
\hline \hline
Logistic model & Best subset logistic model & Lasso penalized logistic model \\ 
\hline
\small donated = SRAmoney & donated = \(\alpha\) + ultimatum2+ SRA3 + SRA4 + SRA10 & donated = \(\alpha\) + ultimatum2 + SRA3 + SRA4 \\
 \hline
 0.24649 & 0.26193 & 0.25177 \\  
 \hline \hline
\end{tabular}
\end{adjustbox}
\end{center}
 
\begin{center}
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{ c | c | c }
\hline \hline
Linear model & Best subset linear model & Lasso penalized linear model \\
\hline
log(donations) = SRAmoney & log(donations) = \(\rho\) + trust2 + ultimatum1 + PGG + & log(donations) = \(\rho\) + avgreturn + cooperation + \\
\small & SRA1 + SRA4 + SRA5 + SRA6 & SRA4 + SRA5 + SRA6 \\
\hline
6.98972 & 6.77793 & 5.25229 \\
\hline \hline
\end{tabular}
\end{adjustbox}
\end{center}

\doublespacing
\vspace{5mm} The first table shows that the model containing only the monetary SRA score is the most accurate model for predicting whether people will donate or not. The second table shows that the lasso penalized model containing three of the game outcomes and three SRA items is the best-performing model for predicting donation amounts. 

\section{Discussion and Conclusion}

Currently, the accumulated literature exploring the external validity of social preference games has mixed results. My thesis provides a systematic approach to this topic, where I elicited decisions in four experimental social preference games along with self-reported social behaviors performed in the past. Most importantly, I compared the lab behavior to a natural field measure that is far-removed from my study.

The overall conclusion is that the social preference games do a poor job explaining the field behavior. This was seen by each model\rq s low R$^{2}$ values and the direction of most of the game coefficients. These poor results are still very interesting, however, because it suggests that social preference games may not generalize field behavior as much as it is anticipated to. Social preference games are thought to at least generally indicate individuals\rq \ pro-social behavior, but with the results from my research, this does not seem to be the case.

However, the games and self-reported measures do a good job in predicting donations behavior. For the logistic model, the MSPE values are around 0.25. This means that the prediction models are randomly guessing donation behavior only a quarter of the time. For the linear model, the MSPE values are around 6, which is a very small error. 

Something to note is that self-reported measures seem to perform similarly, if not better, than the games in both explaining and predicting donations behavior. Tables 3-6 show that the self-reported measures are statistically significant while a majority of the game parameters are not, and each model\rq s R$^{2}$ values were very similar (most notably, the models that includes only the total/monetary SRA score has an R$^{2}$ statistic that is very close to the models that include multiple game variables). In addition, the difference between the MSPE values in all of the estimated prediction models is very small: whether all of the social preference games are used or only the self-reported measures are used, the prediction performances are all similar. It can even be argued that the self-reported measures are a more important tool since the same SRA items were consistently included in the best subsets and lasso-penalized models (items 4, 5, and 6 in the logistic models, and items 3 and 4 in the linear models).

This conclusion has a potential policy implication from a research perspective. Experimental games require a lot of resources: not only is it expensive paying participants for attending the lab session and providing money aligned with the payoffs of the games in order to elicit honest actions, but it also takes a lot of time to program the experimental games. Therefore since self-reported measures seem to explain and predict the field behavior just as well as social preference games, perhaps eliminating the games in favor of survey questions is more efficient.

However, this is just the beginning of a systematic approach to uncovering the external validity of experimental social preference games. There are a few things that further research can include. First, both Galizzi and Navarro-Martinez and my study used (recent) university students who self-selected into the experiments. It would be beneficial to use a different participant pool, since our subjects could be inherently different than the general population\footnote{See Levitt and List for a discussion on student participants.}. Further research should also use more field measures. Galizzi and Navarro-Martinez created five field situations. However, their subjects were likely influenced by the experimenter demand effect. On the other hand, my study included only one field measure, but the measure was not influenced by the experimenter demand effect. Therefore further research should incorporate more field measures that can be theoretically mapped to various behavioral constructs, but are also far removed from the study itself. Lastly, perhaps using other social preference games and/or exploring repeated games could provide further insight into the topic.

Finally, this research may potentially spark interest into future studies into the external validity of other behavioral economics topics where lab experiments are also commonly used. For example, using experiments to study risk preferences and time preferences is common, so it would be intriguing to adopt a systemic approach to explore whether in-lab behavior is correlated to field measures.





\newpage

\bibliography{mybibliography}


%\bibliographystyle{apalike}

% \newpage
%\begin{thebibliography}{9}
%
%\bibitem{AndreoniMiller}
%Andreoni, J., and Miller, J.H. (2002).
%\textit{Giving according to GARP: An experimental test of the consistency of preferences for altruism}.
%Econometrica, 70, 737-53.
%
%\bibitem{Baran}
%Baran, N.M., Sapienza, P., and Zingales, L. (2010).
%\textit{Can we infer social preferences from the lab? Evidence from the trust game}.
%NBER Working Paper 15654.
%
%\bibitem{Benz}
%Benz, M., and Meier, S. (2006).
%\textit{Do people behave in experiments as in the field? Evidence from donations}.
%Experimental Economics, 11, 268-81.
%
%\bibitem{Berg}
%Berg, J., Dickhaut, J.W., and McCabe, K.A. (1995).
%\textit{Trust, reciprocity, and social history}.
%Games and Economic Behavior, 90, 166-93.
%
%\bibitem{Burnham}
%Burnham, T., McCabe, K., Smith, V. (2000).
%\textit{Friend-or-foe intentionality priming in an extensive form trust game}.
%Journal of Economic Behavior \& Organization, 43, 57-73.
%
%\bibitem{Carpenter}
%Carpenter, J.P., Verhoogen, E., and Burks, S. (2005).
%\textit{The effect of stakes in distribution experiments}.
%Economics Letters, 86, 393-98.
%
%\bibitem{CharnessRabin}
%Charness, G., and Rabin, M. (2002).
%\textit{Understanding social preferences with simple tests}.
%Quarterly Journal of Economics, 117, 817-69.
%
%\bibitem{Cherry}
%Cherry, T., Fykblom, P., and Shogren, J. (2002).
%\textit{Hardnose the Dictator}.
%American Economic Review, 92(4): 1218-21.
%
%\bibitem{Englmaier}
%Englmaier, F., and Gebhardt, G. (2010)
%\textit{Free-riding in the lab and in the field}.
%CESifo Working Paper No. 3612.
%
%\bibitem{FehrLeibbrandt}
%Fehr, E., and Leibbrandt, A. (2011)
%\textit{A field study on cooperativeness and impatience in the Tragedy of the Commons}.
%Journal of Public Economics, 95, 1144-55.
%
%\bibitem{FehrSchmidt}
%Fehr, E., and Schmidt, K. (1999).
%\textit{A theory of fairness, competition, and cooperation}.
%Quarterly Journal of Economics, 114, 173-68.
%
%\bibitem{Fisman1}
%Fisman, R., Jakiela, P., and Kariv, S. (2014).
%\textit{The distributional preferences of Americans}.
%NBER Working Paper.
%
%\bibitem{Fisman2}
%Fisman, R., Kariv, S., and Markovits, D. (2007).
%\textit{Individual Preferences for Giving}.
%American Economic Review, 97(5): 1858-76.
%
%\bibitem{Franzen}
%Franzen, A., and Pointner, S. (2013)
%\textit{The external validity of giving in the dictator game: A field experiment using the misdirected letter technique}.
%Experimental Economics, 16, 155-69.
%
%\bibitem{Galizzi}
%Galizzi, M., and Navarro-Martinez, D. (2017).
%\textit{On the external validity of social preference games: a systematic lab-field study}.
%Management Science.
%
%\bibitem{Gintis}
%Gintis, H. (2000).
%\textit{Strong reciprocity and human sociality}.
%Journal of Theoretical Biology, 206, 169-79.
%
%\bibitem{Gneezy}
%Gneezy, U., Haruvy, E., and Yafe, H. (2004).
%\textit{The inefficiency of splitting the bill: A lesson in institutional design}.
%The Economic Journal, 114(495), 265-80.
%
%\bibitem{Goeschl}
%Goeschl, T., Kettner, S.E., Lohse, J., and Schwieren, C. (2015)
%\textit{What do we learn from public good games about voluntary climate action? Evidence from an artefactual field experiment}.
%University of Heidelberg, Department of Economics Discussion Paper 595.
%
%\bibitem{GurvenWinking}
%Gurven, M., Winking, J. (2008).
%\textit{Collective action in action: prosocial behavior in and out of the laboratory}.
%American Anthropologist, 110(2), 179-190. 
%
%
%\bibitem{Henrich}
%Henrich, J., et al. (2005).
%\textit{``Economic Man'' in Cross-Cultural Perspective: Ethnography and Experiments from 15 Small-Scale Societies}.
%Behavioral and Brain Sciences. 28(6): 795?815.
%
%\bibitem{Hermann}
%Hermann, B., Thoni, C., and Gachter, S. (2008).
%\textit{Anti-social punishment across societies}.
%Science, 319, 1362-67.
%
%\bibitem{HillGurven}
%Hill, K., and Gurven, M. (2004)
%\textit{Economic experiments to examine fairness and cooperation among the Ache Indians of Paraguay}.
%In J. Henrich, R. Boyd, S. Bowles, C. Camerer, E. Fehr, and H. Gintis (Eds.),
%Foundations of Human Sociality: Economic Experiments and Ethnographic Evidence from Fifteen Small-Scale Societies. Oxford University Press.
%
%\bibitem{Hoffman}
%Hoffman, E., McCabe, K., Shachat, K., and Smith, V. (1994).
%\textit{Preferences, property rights, and anonymity in bargaining games}.
%Games and Economic Behavior, 7(3): 346-80.
%
%\bibitem{Karlan}
%Karlan, D.S. (2005).
%\textit{Using experimental economics to measure social capital and predict financial decisions}.
%American Economic Review, 95, 1688-99.
%
%\bibitem{Levitt}
%Levitt, S., and List, J. (2007)
%\textit{What do laboratory experiments measuring social preferences reveal about the real world?}.
%Journal of Economic Perspectives, 21(2), 153-74.
%
%
%\bibitem{List}
%List, John. 2006.
%\textit{The behavioralist meets the market: measuring social preferences and reputation effects in actual transactions}.
%Journal of Political Economy, 114(51), 1-37.
%
%
%\bibitem{Parco}
%Parco, J., Rapoport, A., and Stein, W. (2002).
%\textit{Effects of financial incentives on the breakdown of mutual trust}.
%Psychological Science.
%
%\bibitem{Rabin}
%Rabin, M. (1993).
%\textit{Incorporating fairness into game theory and economics}.
%The American Economic Review, 83, 1281-1302.
%
%\bibitem{Ross}
%Ross, L., and Ward, A. (1996).
%\textit{Naive realism in everyday life: Implications for social conflict and misunderstanding}.
%Values and Knowledge, 103-35.
%
%\bibitem{Rushton}
%Rushton, J.P., Chrisjohn, R.D., and Fekken, G.C. (1981).
%\textit{The altruistic personality and the self-report altruism scale}.
%Personality and Individual Differences, 2, 293-302.
%
%\bibitem{Slonim}
%Slonim, R., and Roth, A. (1998).
%\textit{Learning in high stakes ultimatum games: An experiment in the Slovak Republic}.
%Econometrica, 66, 569-96.
%
%
%\bibitem{Voors}
%Voors, M., Turley, T., Kontoleon, A., Bulte, E., and List, J.A. (2012).
%\textit{Exploring whether behavior in context-free experiments is predictive of behavior in the field: Evidence from lab and field experiments in rural Sierra Leone}.
%Economic Letters, 114, 308-311.
%
%
%\end{thebibliography}
%

\clearpage
\appendix

\input{appendixa}
\newpage
\input{appendixb}
\newpage
\input{appendixc}
\newpage
\input{appendixd}




\end{document}