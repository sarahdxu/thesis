\documentclass[12pt]{article}
\usepackage{graphicx,booktabs}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{float}
\usepackage{subfloat}
\usepackage{chngcntr}

\usepackage{fixltx2e}
\makeatletter
\renewenvironment{abstract}{%
    \if@twocolumn
      \section*{\abstractname}%
    \else %% <- here I've removed \small
      %
        {\bfseries \Large\abstractname\vspace{\z@}}%  %% <- here I've added \Large
      %
      \quotation
    \fi}
    {\if@twocolumn\else\endquotation\fi}
\makeatother
\usepackage{caption,subcaption}
\DeclareCaptionSubType*[Alph]{table}
\DeclareCaptionLabelFormat{mystyle}{Table~\bothIfFirst{#1}{ }#2}
\captionsetup[subtable]{labelformat=mystyle}

\graphicspath{ {images/} }
\usepackage[letterpaper, portrait, lmargin=1.5in, rmargin=1.25in, tmargin=1in, bmargin=1in]{geometry}
\usepackage{color}

\renewcommand*\thetable{\Alph{section}.\arabic{table}}
\usepackage{setspace}
\usepackage[toc,page]{appendix}
%\usepackage[titletoc]{appendix}
\usepackage{pdfpages}
\usepackage{booktabs}
\usepackage[nottoc]{tocbibind}
\usepackage{caption}
\captionsetup[figure]{
    position=above,
}
\usepackage{dcolumn}
\usepackage[round]{natbib}
\bibliographystyle{plainnat}
\newcounter{mysubtable}
\usepackage{amsmath}
\usepackage{caption}
\newcommand\modcounter{%
  \refstepcounter{mysubtable}%
  \renewcommand{\thetable}{\thesection.\arabic{table}\alph{mysubtable}}%
}

\newcommand{\appendixnumberline}[1]{Appendix\space}

\let\oldappendix\appendix
\makeatletter
\renewcommand{\appendix}{%
  \addtocontents{toc}{\let\protect\numberline\protect\appendixnumberline}%
  \renewcommand{\@seccntformat}[1]{Appendix~\csname the##1\endcsname\quad}%
  \oldappendix
}
\makeatother

%\makeatletter
%%% The "\@seccntformat" command is an auxiliary command
%%% (see pp. 26f. of 'The LaTeX Companion,' 2nd. ed.)
%\def\@seccntformat#1{\@ifundefined{#1@cntformat}%
%   {\csname the#1\endcsname\quad}  % default
%   {\csname #1@cntformat\endcsname}% enable individual control
%}
%\let\oldappendix\appendix %% save current definition of \appendix
%\renewcommand\appendix{%
%    \oldappendix
%    \newcommand{\section@cntformat}{\appendixname~\thesection\quad}
%}
%\makeatother

    \usepackage{tabularx}
\usepackage{adjustbox}




\doublespacing

\begin{document}

\includepdf[pages=-]{titlepage.pdf}

\newpage
\onehalfspacing
\tableofcontents
\thispagestyle{empty}


\newpage

\clearpage
\pagenumbering{arabic} 
\doublespacing
\section*{Acknowledgements}
\addcontentsline{toc}{section}{Acknowledgements}
First and foremost, I would like to express my thanks to my mentor, Professor Naecker. It has been a privilege to be your research assistant, student, and thesis advisee. I have learned so much from you over the past two years: from economics research and econometric techniques to computer programming. Thank you for your guidance, support, and time -- words cannot express my gratitude.\\

Thank you to Linda Mascaro, Rhoanne Esteban, and the Wesleyan Economics department for all of the help you have provided me in writing this thesis. I am extremely grateful that Wesleyan has supportive people like you. \\

Lastly, I would like to thank my family for their unending support. Jay, thank you for your words of encouragement - I wouldn\rq t have written this thesis if it weren\rq t for you. Thank you, Jenny, for being my foundation. I appreciate the time you took to read over my thesis, and for always being there for me. Thank you Mom and Dad for giving me the opportunity to have the incredible educational experience of studying at Wesleyan. Mom, your limitless love and positive outlook on life, and Dad, your work ethic and strength, have shaped me to be the person I am today. I am incredibly lucky to be surrounded by such amazing people. You are my greatest role models.


\newpage

\section*{Abstract}
\addcontentsline{toc}{section}{Abstract}


The external validity of experimental games is a growing topic of interest because researchers commonly use such games to study social preferences. Many papers have conducted within-subjects experiments, comparing in-lab game decisions to real-world decisions. However, the current accumulated evidence has yielded mixed conclusions. Some papers have found statistically significant associations between game and field behavior, while other papers have not. In this research, I compared behavior in various social preference games and self-reported measures regarding past pro-social behavior to a natural, far-removed field measure: donations to Wesleyan University. Donations were recorded as binary (whether individuals donated or not) and continuous (individuals\rq \ average yearly donations) metrics. To examine the explanatory power of social preference games on donations behavior, I ran several logistic and linear regression models. I also compared how well the games predict donations behavior relative to a baseline model. My results show that the social preference games do a poor job explaining donations behavior. On the other hand, the games have modest additional predictive power given the baseline model\rq s predictions. Most notably, the self-reported measures seem to perform just as well, if not better, than the social preference games in both explaining and predicting the field measure. 



\newpage


\doublespacing
\section{Introduction and Literature Review}

The standard economic model assumes that individuals\rq \ actions are motivated purely by self-interest. However, from simple observation it is clear that people also care about the well-being of others: people volunteer\footnote{The Corporation for National \& Community Service reported 63.6 million volunteers and 7.9 billion hours of service in the U.S. in 2015.}, people donate\footnote{According to Giving USA, individual giving in 2016 was reported to be \$281.86 billion, a 3.9 percent increase from the previous year.}, and social welfare programs exist. If someone is not solely motivated by material self-interest but also cares about the well-being of others, we say that they have social preferences. 

A common approach in studying social preferences is to conduct experiments in a laboratory setting. Subjects play experimental games, where they receive monetary incentives. The benefit is that researchers are able to control prices, information, and actions available to the participants, allowing researchers to rigorously target different aspects of social behavior\footnote{Many papers have identified various aspects of social preferences such as altruism, social welfare, inequality aversion, and reciprocity. See, e.g., \cite{charness_rabin_2002}; \cite{rabin_1993}; \cite{fisman_jakiela_kariv_2014}.}.

The study of social preferences with experimental games goes back over twenty years. \cite{kahneman_1986} recruited undergraduate students to play the first dictator game experiment in economics. The dictator game involves a single player (the dictator) who receives an endowment and must then choose how to split the amount between themselves and a second player (the receiver). In their study, the dictator was given two choices on how to split the endowment: either an even split or an uneven split. The authors found that a majority of the subjects favored the equal split option, contrary to the basic economic theory of self-maximizing interests. \cite{forsythe_1994} also recruited undergraduate students from the University of Iowa to play the dictator game, but this time the dictators had free range over how to split the endowment. They found that the average contribution was about 20\% of the endowment.

Forsythe et al.\ additionally had subjects play the ultimatum game. The ultimatum game is a two-player game in which two players, a proposer and a responder, bargain over a fixed endowment: the proposer divides the endowment, and the responder either accepts or rejects the offer. If accepted, the proposal is implemented; if rejected, both players receive nothing. The authors found that
%, contrary to the selfish subgame perfect Nash equilibrium in which proposers and responders do not send any money, 
most proposers shared their endowments and responders rarely rejected offers.

\cite{berg_1995} recruited undergraduate students from the University of Minnesota to play the trust game. Similar to the ultimatum game, the trust game is a two-player game in which the proposer proposes how to divide the given endowment between themselves and the responder. The offered amount is multiplied by a factor, and the responder decides how much of the multiplied contribution to return to the proposer. The authors found that participants showed high levels of trust and reciprocity: 94\% of the proposers sent money, and over one-third of the responders returned an amount greater than what was given to them.

\cite{marwell_1981} conducted public goods experiments with high school and college students in Madison, Wisconsin. The public goods game is an N-player game where each player receives the same sum of money and simultaneously decides how much to contribute into a public fund. The total amount in the public fund is multiplied by a factor, and divided evenly amongst the players. The authors found that subjects generally sent 40\%--60\% of their endowment, showing `weak' free riding behavior: the amount contributed into the public fund was between the Pareto efficient level (contributing the entire endowment) and the free riding level (contributing nothing).

% \cite{hermann_thoni_gachter_2008} conducted public goods experiments across 16 participant pools. The public goods game is an N-player game where each player receives the same sum of money and simultaneously decides how much to contribute into a public fund. The total amount in the public fund is multiplied by a factor, and divided evenly amongst the players. Hermann et al. found that varying opportunities for punishment led to varying cooperation levels.

Using social preference games to infer real-world behavior has become one of the building blocks of experimental and behavioral economics. However, pro-social behavior may be influenced by non-monetary factors, leading to variation in game behavior. One such factor is that the subjects\rq \ actions are often under the scrutiny of the researcher. \cite{hoffman_1994} found that almost half of their subjects donated at least \$3 (out of a \$10 pie) when playing the dictator game. However, when the authors implemented a ``double-blind'' treatment where both the experimenter and other subjects could not observe the dictators\rq \ actions, only 16\% of subjects gave at least \$3.

Individuals may also be influenced by the framing and context of the situation. \cite{burnham_mccabe_smith_2000} conducted the trust game in which they switched between calling the responder as ``partner'' or ``opponent''. The authors found that trustworthiness with a ``partner'' was over twice than that for an ``opponent''. Similarly, \cite{ross_ward_1996} found that participants showed higher levels of cooperation when playing a prisoner\rq s dilemma game called a ``community'' game and lower levels when the game was called a ``Wall Street'' game. 

%Varying the level of stakes can also lead to significantly different lab behavior. \cite{carpenter_verhoogen_burks_2005} found that increasing the stakes from \$10 to \$100 decreased the median offer in the dictator game from 40\% of the endowment to 20\%. In the ultimatum game, \cite{slonim_roth_1998} found that rejections occurred less frequently and proposal amounts decreased as the stakes increased. On the other hand, \cite{cherry_frykblom_shogren_2002} found no significant difference in offer amounts when increasing the stakes from \$10 to \$40 in the dictator game. 

These examples demonstrate that non-monetary factors can yield significant variations in lab behavior. On a larger scale, experimental games conducted in a lab setting are abstract and remote from realistic situations; such non-monetary factors are expected to also lead to differences in behavior between a lab setting and a real-world setting. For example, people know that their actions are being watched and recorded during an experiment, but their decisions are usually made in private when making real-life choices. Similar to the study by Hoffman et al.\ where higher levels of scrutiny led to more pro-social behavior, people may behave more pro-socially in the lab for the same reason than they are in real life. Another example is that in the studies by Burnham et al.\ and Ross and Ward, subjects were more pro-social when the games were framed collaboratively -- that is, the context of the situation is important. Likewise, individuals\rq \ actions in the lab may not reflect their real-world pro-social behavior since real-life decisions may hold different personal meaning to the individual than experimental games do. Indeed, there are many non-monetary factors that can lead to deviations in behavior\footnote{See \cite{levitt_list_2007} for a full discussion of non-monetary factors, with supporting literature.}. Therefore, an important question to ask is the external validity of social preference games -- to what extent can decisions made in experimental games be generalized to decisions made in the field?

Some studies have concluded that lab behavior does explain field behavior. \cite{baran_2010} recruited MBA alumni and found that their reciprocity behavior when playing the trust game predicted their donations to their university. \cite{franzen_pointner_2012} compared decisions from university students participating in dictator games to their actions when receiving a misdirected letter containing money; subjects who showed pro-social behavior in the lab returned the letters more often than subjects who were selfish in the lab. \cite{englmaier_gebhardt_2011} conducted a field experiment where they compared university students\rq \ free riding behavior at the library to free riding behavior in a public goods game. The authors found a statistically significant correlation between the field and lab measures.

There are also studies that used non-student subjects. \cite{fehr_leibbrandt_2011} recruited Brazilian fishermen to play the public goods game, and found that fishermen who were more cooperative in the games were less likely to exploit the communal fishing grounds. \cite{karlan_2005} compared behavior to loans repayment behavior in a Peruvian microfinance program: subjects who displayed ``trustworthy'' behavior were less likely to default on their loans. 

However, a number of papers found that lab behavior had no explanatory power over field behavior. \cite{goeschl_2015} examined university students\rq \ actions in two tasks: the public goods game and contributions to a field situation about reducing CO$_{2}$ emissions. The authors found that behavior in both tasks were uncorrelated. \cite{hill_gurven_2004} carried out the ultimatum game and public goods game on Paraguay Ache Indians. They compared the game decisions to observed food production and sharing patterns with individuals outside of the nuclear family, and found no significant relationship. \cite{gurven_winking_2008} recruited Tsimane forager-horticulturalists in Bolivia, and compared the behavior when playing the dictator game and ultimatum game to their food-sharing behavior. The authors likewise found no relation between the two measures. \cite{voors_2012} studied farmers in Sierra Leone, and found no meaningful correlation in behavior between actions in the public goods game and actions when asked to contribute to a real community public good.

\cite{galizzi_navarro-martinez_2017} summarized that about 40\% of reported correlations between experimental games and field behavior found statistically significant associations\footnote{See Galizzi and Navarro-Martinez for their full systematic review and meta-analysis of literature on within-subjects studies comparing lab and field behavior.}. Therefore it is clear that the current evidence for the external validity of experimental games is mixed. The authors argued, however, that the previous studies compared only one social preference game to one specific field measure -- it is crucial to have a more systematic approach. The authors proposed using multiple experimental games and self-reported questionnaires to compare to multiple field responses. 

Galizzi and Navarro-Martinez conducted their own study in which participants answered questions about social behaviors exhibited in the past, played various social preference games, and then encountered naturalistic field situations. The possible field situations included a research assistant asking for help carrying boxes down the stairs, asking to use the participant\rq s phone to make a brief phone call, asking for donations to a children\rq s charity, asking for donations to an environmental charity, or asking for donations to the lab\rq s research fund.  The authors\rq \ overarching conclusion was that the experimental games did a poor job of explaining the field behavior. However, they asserted that more systematic studies like theirs are needed in order to draw a definite conclusion. This is where my thesis contributes.

My research provides another systematic study to add to the ongoing discussion on the external validity of social preference games. I recruited Wesleyan University seniors and recent alumni (those who graduated within the last five years) to participate in various experimental games. I also collected self-reported questionnaires regarding past pro-social behavior as a supplementary layer to analyze the explanatory and predictive ability of the games.

In order to further inform my experiment design, it is critical to think about why some previous studies found correlations between the experimental game and field measures while other studies did not. While playing the experimental games for the study by Hill and Gurven, participants expressed worry that their choices would upset the receiver. Given that the tribal group\rq s culture was heavily focused on community, perhaps the participants didn\rq t want to risk making any choices that would create any tension. Likewise, in the studies by Gurven and Winking and Voors et al., participants were subject to high scrutiny and low anonymity since it was easy for community members to find out the choices each subject made. It is possible that such factors skewed results. Therefore an important feature in my research design was that participants played the games remotely. This set-up ensured minimal scrutiny and higher anonymity, reducing the possibility of influenced behavior.

When deciding on my field measure, one criticism of the experimental design by Galizzi and Navarro-Martinez is how the authors executed their field measures. The field situations occurred as the participants were exiting the lab session, so it is likely that the participants were able to connect the encounters to the research lab, especially since both events were about pro-social behavior and occurred in such close time proximity. This may have led to actions that participants would not have done if they were unaware of the scrutiny. This is called the experimenter demand effect, which is a common problem in experimental design: if participants know their actions are being examined by the researcher, they may change their behavior towards what they think the researcher wants. Therefore, it is more desirable to use a natural, far-removed situation as the field measure: in this research, I used the participants\rq \ donations to Wesleyan University.

Overall, my thesis adopted the systematic approach of Galizzi and Navarro-Martinez while refining and enhancing the experimental and field responses to reduce the experimental response bias. I first examined how well the social preference games explain donations. Donations were coded both as whether the participant had ever donated (binary) and the participant\rq s average yearly donations (continuous), so I ran several logistic and linear regression models. I found that the games explain only a small portion of the variation in donations behavior. In fact, the self-reported measures explain the field measure just as well as the games, as seen by the R$^{2}$ values from the regression models. I then analyzed how well the games and self-reported measures predict donations. I used the lab behavior from older alumni class year participants to build prediction models, and the donations behavior from the newest alumni class year participants to test the models\rq \ performance. I found that the games and self-reported measures do a good job of predicting if individuals will donate: prediction accuracy was increased by 15\% compared to the baseline model. However, the games and self-reported measures did not add any value for predicting the amount donated given the baseline model. In addition, the self-reported measures seem to play a more important role than the social preference games in predicting the field behavior.


\section{Methods}
 
Wesleyan University seniors and recent alumni received an email that explained my research and invited them to participate in my study. 2,004 emails were provided by Wesleyan University Relations. Participants were informed that upon their completion of the study, their college major, class year, and Wesleyan donations information would be collected.

Each participant was presented with two tasks to complete: (i) incentivized social preference games, and (ii) self-reported questionnaires regarding past pro-social behavior. Online links to both the experimental games and survey questions were included in the email\footnote{The entire study was computerized, programmed and implemented using Qualtrics. Those who chose to take part completed the study remotely.}. The field measures used to compare to the participants\rq \ lab results were their donation status (whether they have ever donated) and average yearly donations to Wesleyan University, provided by Wesleyan University Relations\footnote{All donations measures were provided as of January 2018.}.
%In addition, I examined which tool -- the social preference games, the self-reported measures, or both -- performed better at predicting the donations behavior.

To incentivize participation as well as elicit honest game behavior, participants were informed that completing the entire study made them eligible for a lottery prize. Each social preference game used ``tickets" as the experimental currency unit, where the total tickets each participant earned in the games equaled how many lottery tickets they owned. At the end of the study, ten tickets were randomly drawn, and the winners were contacted and instructed on how to receive their \$100 prize.


\subsection{Incentivized social preference games}

%Those who chose to participate in the study received a unique identifier assigned by Qualtrics.

Participants first played four social preference games (in random order): the generalized dictator game, the ultimatum game, the trust game, and the public goods game. They received detailed instructions for each game along with an example to illustrate how the game worked. For the ultimatum game and the trust game, participants played as both the proposer and responder in separate rounds. At the end of the participation deadline, all participant responses were randomly paired and ticket payoffs were calculated.

Below are descriptions of each game the participants played, along with examples. See Appendix C for screenshots of each game, along with respective instructions and examples, from the online study. In addition, I describe the decisions individuals are expected to have made depending on the various preferences they may have.

\subsubsection{Generalized dictator game}

Each participant played nine different rounds of the generalized dictator game. In each round, the participants were given various endowments of tickets and prices of giving, and were asked how they would like to divide the endowment with an anonymous, random Player 2. The budget constraint is given by \(\pi_{s} + p\pi_{o} = \textit{m}\), where \(\pi_{s}\) is how many tickets were kept, \(\pi_{o}\) is how many tickets were given, \(p\) is the relative price of giving, and \textit{m} is the endowment. The endowment in each round was either 10, 12, or 15 tickets. Every ticket the participant kept was multiplied by 1, 2, 3, or 4 (hold price), and every ticket given to Player 2 was multiplied by 1, 2, 3, or 4 (pass price) so that the price of giving was \(\frac{hold \ price}{pass \ price}\). Below are the nine sets of endowments and prices of giving used in the study, based on the design by \cite{andreoni_miller_2002}:

\begin{center}
\begin{tabular}{ c c c c c }
\hline \hline
 Round & Ticket Endowment & Hold Price & Pass Price & Relative Price of Giving \\ 
 \hline
1 & 15 & 1 & 2 & \(\frac{1}{2}\)  \\  
2 & 10 & 1 & 3 & \(\frac{1}{3}\)  \\  
3 & 15 & 2 & 1 & 2 \\  
4 & 12 & 1 & 2 & \(\frac{1}{2}\)  \\  
5 & 10 & 3 & 1 & 3  \\  
6 & 15 & 1 & 1 & 1  \\  
7 & 12 & 2 & 1 & 2 \\  
8 & 10 & 4 & 1 & 4 \\  
9 & 10 & 1 & 4 & \(\frac{1}{4}\)  \\ 
\hline \hline \\
\end{tabular}
\end{center}

Participants were given the endowments and prices for each round, and were presented an interactive slider bar that displayed the different ticket amounts each player would earn. The slider bar ensured that all choices fulfilled the budget constraint, and eliminated the need for participants to perform their own calculations by directly showing how many tickets each player would receive.

\textit{Example:} Suppose the endowment was 10 tickets, the hold price was 1, and the pass price was 3 (so that the relative price of giving was \(\frac{1}{3}\)). That is, however many tickets the player decided to keep was multiplied by 1, and however many tickets given to Player 2 was multiplied by 3. The player was presented a slider bar, ranging from giving 0 tickets to giving 10 tickets. With each possible offer amount, the player was told the total ticket amounts received by both players in the form (total tickets received, total tickets given). If the player decided to give 4 tickets to Player 2, they saw (6, 12) on the slider bar; they would receive 6 tickets (10 - 4 = 6), and Player 2 would receive 12 tickets (4 * 3 = 12). See Figure C.1 in Appendix C for images of the generalized dictator game from the online study.

The utility function for individuals with selfish preferences is represented by \(U(\pi_{s}\), \(\pi_{o}\)) = \(\pi_{s}\). Selfish individuals will prefer to only maximize their own payoff, so their optimal allocation is to always keep their entire endowment.

The utility function for Rawlsian preferences is given by \(U(\pi_{s}\), \(\pi_{o}\)) = \(\min(\pi_{s}, \pi_{o})\). If the individual has Rawlsian preferences, their optimal allocation would be to equalize the final number of tickets. For example, if the price of giving was 1 and the endowment was 10 tickets, \(\pi_{s}\) = \(\pi_{o}\) = 5 tickets. Or if the price of giving was \(\frac{1}{2}\) and the endowment was 15 tickets, the dictator would give 5 tickets: they would receive 10 tickets and the second player would receive 5 * 2 = 10 tickets. 

Individuals with utilitarian preferences have the utility function \(U(\pi_{s}\), \(\pi_{o}\)) = \(\pi_{s}\) + \(\pi_{o}\). Such individuals would allocate all payoffs to whichever \(\pi_{s}\) or \(\pi_{o}\) is cheaper. That is, if \(p\) was less than 1, they would send their entire endowment to the second player, and if \(p\) was greater than 1, they would keep the full endowment.


\subsubsection{Ultimatum game}

\textbf{Player 1:} Each participant was endowed with 10 tickets, and was instructed to decide how much of their endowment to send to an anonymous, random responder (Player 2) so that \(\pi_{s} + \pi_{o} = 10\). They were informed that the responder may or may not reject the proposed allocation: if the allocation was accepted, then the proposal was implemented (Player 1 received \(\pi_{s}\) and Player 2 received \(\pi_{o}\)), but if the allocation was rejected, neither player received any tickets.

\textit{Example:} Suppose the participant chose to give 3 tickets to Player 2. If Player 2 accepted the proposal, the participant received 7 tickets (10 - 3 = 7) and Player 2 received 3 tickets. However, if Player 2 rejected the proposition, then both players received 0 tickets. See Figure C.2 in Appendix C for screenshots of the Player 1 version of the ultimatum game from the online study.
 
\textbf{Player 2:} Each participant was informed that the proposer (Player 1) was given an endowment of 10 tickets. They were then presented a list from 0 tickets to 10 tickets (in 1-ticket increments), which represented the different amounts that Player 1 could choose to send. The participants were instructed to indicate whether they accept or reject each hypothetical proposal, and were told that accepting means they agree to receiving the offered amount, and rejecting means they do not agree and instead both players will receive 0 tickets.

\textit{Example:} Suppose the participant chose to reject all offered amounts below 5 tickets and accept all offered amounts equal to or greater than 5 tickets. If Player 1 chose to offer 6 tickets, then Player 1 received 4 tickets (10 - 6 = 4) and the participant received 6 tickets. If Player 1 chose to offer 3 tickets, on the other hand, then both players received 0 tickets. See Figure C.3 in Appendix C for images of the Player 2 version of the ultimatum game from the online study.

Since selfish individuals seek to maximize their own payoffs, selfish responders should never reject a non-zero offer. If selfish proposers assume responders are also selfish, they would offer the smallest non-zero amount (in this case, 1 ticket). The exception would be if selfish proposers think the responder would be indifferent between being offered 1 ticket or 0 tickets, i.e.\ the responder would accept 0 tickets. In this case, the proposer would offer 0 tickets.

%However, if the proposers believe the responders are not purely selfish and will reject small amounts, then they will offer the smallest non-zero amount they believe will be accepted. For example, if the proposer believes the responder has Rawlsian preferences (i.e. the responder will reject offers below half the endowment) then the proposer will offer 5 tickets.

However, responders may also care about both inequality and total welfare. They may have Fehr-Schmidt difference-aversion preferences\footnote{See \cite{fehr_schmidt_1999}.}:\\

$ U(\pi_{s}, \pi_{o})  = \begin{cases}
      \pi_{s} - \alpha(\pi_{s} - \pi_{o}) & \text{if }\pi_{s} > \pi_{o} \\
      \pi_{s} - \beta(\pi_{o} - \pi_{s}) & \text{if }\pi_{s} \leq \pi_{o}
    \end{cases}\, $ \\

\noindent where $ 0 \leq \alpha \leq \beta \leq 1$. That is, the responder dislikes inequality but dislikes it even more if they have the smaller allocation. For example, say $\alpha = \frac{1}{2}$ and $\beta=1$. From the point of view of responder (i.e.\ they are Player 1 so they receive \(\pi_{s}\): compared to the utility of rejecting, \((U(\$0, \$0) = 0)\), the responder would accept payoffs \((\$4, \$6)\) since \(U(\$4, \$6) = 4 - 1(6 - 4) =  2\), but reject payoffs \((\$3, \$7)\) since \(U(\$3,  \$7) = 3 - 1(7 - 3) = -1\).

	
\subsubsection{Trust game}
\textbf{Player 1:} Each participant was endowed with 10 tickets. They were prompted to decide how much of their endowment to send to an anonymous, random responder (Player 2) so that \(\pi_{s} + \pi_{o} = 10\). The participants were also told that the amount sent over would be multiplied by 3, i.e.\ Player 2 would receive 3\(\pi_{o}\). Player 2 would then decide how many tickets, r \(\leq 3\pi_{o}\), they would like to return. Overall, Player 1 received \(\pi_{s}\)+ r tickets and Player 2 received 3\(\pi_{o}\) -- r tickets.

\textit{Example:} If the participant chose to send 5 tickets, then Player 2 received 15 tickets (5 * 3 = 15). If Player 2 returned 3 tickets, then Player 1 earned a total of 8 tickets (5 + 3 = 8) and Player 2 earned a total of 12 tickets (15 -- 3 = 12). See Figure C.4 in Appendix C for images of the Player 1 version of the trust game from the online study.

\textbf{Player 2:} Each participant was informed that the proposer (Player 1) received an endowment of 10 tickets. They were then given a list of all ten possible multiplied amounts that Player 1 could have chosen to send, ranging from 3 tickets to 30 tickets in 3-ticket increments. For each possible amount offered, the participants entered the number of tickets they would like to return back to Player 1. They were given clear instructions that their input could not exceed what was given to them. For example, if Player 1 sent a multiplied amount of 15 tickets, then the maximum number of tickets the participant could return was 15 tickets.

\underline{\textit{Example:}} Suppose the participant\rq s return scheme was as below: \\

\begin{center}
\begin{tabular}{ c c }
\hline \hline
 Possible Offer Amounts (Multiplied) & Tickets Returned \\ 
 \hline
3 & 0  \\  
6 & 1 \\  
9 & 3  \\  
12 & 3  \\  
15 & 5   \\  
18 & 5  \\  
21 & 5 \\  
24 & 5 \\  
27 & 5 \\  
30 & 5 \\  
\hline \hline \\
\end{tabular}
\end{center} 


\noindent where the first column is a list of all possible multiplied amounts that Player 1 could choose to send, and the second column indicates how many tickets the participant would like to return. If Player 1 decided to send 7 tickets, then the participant received a multiplied amount of 21 tickets (7 * 3 = 21). As indicated by the participant\rq s return scheme, the participant chose to send back 5 tickets. Thus Player 1 earned a total of 8 tickets (3 + 5 =  8), and the participant earned a total of 16 tickets (21 -- 5 = 16). See Figure C.5 in Appendix C for images of the Player 2 version of the trust game from the online study.

Similar to the ultimatum game, selfish responders should return nothing; if selfish proposers assume responders are also purely selfish, then they are expected to pass nothing. However, proposers with utilitarian preferences prefer to maximize the sum of payoffs. Since the amount sent to the responder is multiplied by a positive factor, they are expected to send everything to the responder. This can also be seen mathematically: \(U(\pi_{s}, \pi_{o}) = \pi_{s} + \pi_{o} = \pi_{s} + r + 3\pi_{o} - r = \pi_{s} + 3\pi_{o} \). The amount sent to the responder increases the proposer\rq s utility more than the amount kept, and so the proposer should choose to send their entire endowment to the responder.
	
\subsubsection{Public goods game}

Participants were each given 10 tickets, and were told that they would be randomly matched with one other player. They were then prompted to decide how much of their endowment to contribute to a public fund, and that the other player was told to do the same task. The total tickets in the public fund was multiplied by 2, and divided evenly between the two players.

The payoff function is given by \( P_{i} = 10 - g_{i} + \sum_{n=1}^{2} g_{n}\) where \(g_{i}\) is the amount that player \textit{i} donated to the fund, and \(\sum_{n=1}^{2}g_{n}\) is the sum of both players\rq \ donations to the public fund. Therefore Player 1\rq s payoff function is \( P_{1} = 10 - g_{1} + \sum_{n=1}^{2} g_{n} = 10 + g_{2}\), and likewise Player 2\rq s payoff function is \( P_{2} = 10 - g_{2} + \sum_{n=1}^{2} g_{n} = 10 + g_{1}\).

\textit{Example:} Suppose the participant contributed 3 tickets, and the second player sent 5 tickets. The total donated amount, \(\sum_{n=1}^{N} g_{n}\), was 8 tickets. Therefore the participant received \( P_{1} = 10 + 5  = \) 15 tickets, and the second player received \( P_{2} = 10 + 3 = \) 13 tickets. See Figure C.6 in Appendix C for screenshots of the public goods game from the online study.

As seen in the payoff functions, each player\rq s payoff depended on the other player\rq s contribution. Therefore selfish players should not send any tickets into the public fund. But if the player has utilitarian preferences, from the perspective of Player 1, their utility is: \(U_{1} = P_{1} + P_{2} =  20 + g_{1} + g_{2}\). Maximizing their utility would depend on both players\rq \ contributions, so individuals with utilitarian preferences should send their entire endowment into the public fund.


\subsubsection{Strategy Method}

Participants completed the games at their own availability, and their responses were randomly matched with other participants\rq \ responses after all data had been collected. In the two-player games which required both a proposer and responder (the ultimatum game and the trust game), each participant played both roles. When playing as the responder, the participant saw a list of all possible offer amounts and was prompted for their choice given each offer. This method of conducting an experiment is called the strategy method.

There are several reasons why the strategy method is advantageous over matching participants live. First, it is useful to have all of the responder\rq s potential choices so that payoffs can be calculated after the fact. Most importantly, the strategy method simply gives more information. If players were matched with other players live, then we would only have the responder\rq s response given the proposer\rq s sole offer. The strategy method, however, provided all returned amounts for all possible donated amounts in the trust game, and provided participants\rq \ minimum accepted amount in the ultimatum game. 

\subsubsection{Payment}

After all experimental data was collected, participants\rq \ responses were randomly matched. In each pair, one participant was randomly assigned to their Player 1 choices (i.e.\ they were the dictator in the generalized dictator game, and proposer in the ultimatum game and the trust game), and the other was assigned to their Player 2 responses (i.e.\ they were the receiver in the generalized dictator game, and the responder in the ultimatum and the trust game). For each pair of participants, I ran through each of the four games and calculated ticket payoffs. Since the generalized dictator game consisted of nine rounds, one round was randomly selected, and participants received tickets from that selected round. 

Once all of the participants\rq \ total lottery tickets were allocated, ten tickets were randomly drawn for the prize. Each winner was emailed instructions on how to receive their \$100 prize through direct deposit or by check. 

\subsection{Self-reported measures of past pro-social behaviors}

Participants then reported on past pro-social behavior. The questions the participants were asked were adapted from the Self-Report Altruism (SRA) scale introduced by \cite{rushton_chrisjohn_fekken_1981}. The survey was comprised of 10 items, and participants were instructed to report how frequently they have done each item. Participants rated each of the 10 statements as either ``Never'', ``Once'', ``More than once'', ``Often'', or ``Very often''. Examples included ``I have donated money at the cash register when buying groceries'' and ``I have pointed out a clerk\rq s error (at the supermarket, at a restaurant) in undercharging me.'' A full list of the items from the online study is displayed in Appendix D. 

\subsection{Field Measure}

Wesleyan University Relations provided a dataset that contained each participant\rq s donation status (whether they had ever donated) and cumulative donations to Wesleyan as of January 2018. I normalized each participant\rq s total donation amount by dividing by the number of years they have been solicited, which resulted in the participant\rq s average yearly donations.

There are several reasons why I used donations as my field measure. First, many behavioral economics papers that explored the relationship between lab behavior and field behavior used donations as their field measure, so there is strong precedent\footnote{See, e.g., \cite{benz_meier_2006}; Baran et al.; Falk et al.}. \cite{falk_2013} justified that donations are an accurate field measure because they do not rely on self-reported responses but on actual decisions. Furthermore, donations are often made in private and the amounts are usually never made public, and students/alumni are unaware that their actions may be analyzed in future research. The lack of scrutiny during the act of donating therefore indicates that donations should reflect the donors\rq \ genuine altruism. The authors then pointed out the most important reason for their decision to use donations: all students and alumni are solicited, so everybody has to make the decision about donating.

Donations are a more accurate field measure than creating a scenario at the end of the online session. In the experiment by Galizzi and Navarro-Martinez, participants were subject to the experimenter demand effect: because participants encountered the field experiments shortly after completing the lab experiments, it would not be difficult for participants to link the two situations together. Therefore, participants could have acted more pro-socially than they normally would. At Wesleyan University, seniors are solicited for donations around once per semester; alumni are solicited at the end of November or beginning of December, and again in March and June. The online study in this research was released between solicitation cycles (early January 2018), and participants\rq \ donation measures were provided as of January 2018. Therefore participants\rq \ donation were not influenced by the experimenter demand effect.


\subsubsection{Theoretical Correspondences Between Games and Donations}

All four of the social preference games used in my study tapped into different aspects of pro-social behaviors that are related to donating. The first movers in each game 
%the dictators in the generalized dictator game, proposers in the ultimatum game and trust game, responders in the trust game, and players in the public goods game 
all had the option to keep their endowment to themselves, but may also have chosen to share their endowment with the other player. Therefore the actions in each of the four games can be explained by altruism: those who chose to keep their endowment exhibited lower levels of altruism, and those who chose to share their endowment demonstrated higher levels of altruism. Likewise, altruism is the most common representation for donating since the donors chose to give a portion of their income to the university to benefit future students.

Proposers\rq \ decisions in the trust game indicated their trust levels. The offered amount was multiplied by a factor, so they could exhibit trust by sharing their endowment with the hopes that the responders would send back an amount larger than what was given. Correspondingly, in the real world, donating may also represent trust since donors have trusted that the University would not misappropriate the funds.

Responders\rq \ actions in the trust game can be explained by reciprocity. If the proposers decided to share their endowment, then the responders had the opportunity to give back. The responders displayed higher levels of reciprocity if they returned tickets, and displayed lower levels of reciprocity if they did not. 
%The responders\rq \ actions can also be explained by trustworthiness: the proposers may contribute some of their endowment in hopes of the responders returning a larger amount than what was offered. Thus responders who return tickets indicate higher levels of trustworthiness. 
Equivalently, donations may be explained by reciprocity since seniors and alumni may donate to the university as a thanks for providing them with invaluable resources and opportunities throughout their educational experience.

Players\rq \ decisions in all of the games can also be explained by inequality aversion. They may have chosen to share their endowment so that the other player would be left with nothing. Donations could also potentially be explained by inequality aversion: people may have donated because they believe their donations would help students receive opportunities they would not be able to obtain otherwise. Donors also have the option to target what area their donations can go towards: for example, if the donor directed their donations to Financial Aid, then they displyed inequality averse preferences.

Lastly, players\rq \ actions in the public goods game could be explained by their cooperation levels. Players may have chosen to contribute a portion of their endowment because they believed the other players were doing the same thing. In the case for donations, people could exhibit cooperation by donating because the University solicited for donations (i.e.\ they are cooperating with what the University is asking for), or because they believe other seniors/alumni donated.

	
\subsection{Participants and Sessions}

Wesleyan University Relations provided a random sample of 2,004 emails (334 emails for each class year from 2013 to 2018), and I sent an invitation email in early January 2018 that requested for participation in my study. Participants were informed that the study consisted of several experimental games and non-incentivized survey questions. The participants were volunteers who opened the link to the online study, provided consent for me to receive their major, class year, and Wesleyan donations data, and completed both the social preference games and survey questions. The deadline for participation was mid-February 2018; a total of 397 participants completed the online study. Shortly after the participation deadline, all participants\rq \ responses were randomly paired, ticket payoffs were calculated, and 10 winners were randomly selected\footnote{Since there was an odd number of participants, one participant was randomly selected to be paired up twice. They received the ticket payoffs from only their first pairing.}. The winners were emailed in the beginning of March 2018 with instructions on how to receive their prizes.

\section{Results}
The results are presented in four distinct sections. I begin by describing the results obtained for the three main elements of my research (social preference games, self-reported measures, and donations). The last section focuses on answering the main question of this paper: the extent to which the games explain and predict donations behavior. Appendix A contains all figures and tables; Appendix B contains the variables definition table.

\subsection{Social Preference Games}
Since the generalized dictator game consisted of 9 rounds with varying sets of endowments and prices of giving, I assumed each participant\rq s giving preferences was a member of the constant elasticity of substitution (CES) utility function\footnote{Andreoni and Miller and \cite{fisman_kariv_markovits_2007} used the CES utility function to represent subjects\rq \ preferences.}.  The CES utility function is written as: \(U_{s} = [\alpha(\pi_{s})^{\rho} + (1-\alpha)(\pi_{o})^{\rho}]^{1/\rho}\).

The first parameter, \(\alpha\), measures the relative weight on the payoff to self. Holding \(\rho\) constant, as \(\alpha \rightarrow 1\), \(U_{s} \rightarrow \pi_{s}\), i.e.\ the individual\rq s utility depends only on the amount they keep for themselves. As \(\alpha \rightarrow 0\), \(U_{s} \rightarrow \pi_{o}\), i.e.\ the individual\rq s utility depends only on the amount the other person receives. Therefore as \(\alpha \rightarrow 1\), the individual exhibits selfish preferences, and as \(\alpha \rightarrow 0\), the individual exhibits selfless preferences.

The second parameter, \(\rho\), indicates the willingness to trade off payoffs to self and other in response to price changes. Holding \(\alpha\) constant, as \(\rho \rightarrow 1\), \(U_{s} \rightarrow \alpha\pi_{s} + (1-\alpha)\pi_{o}\),  i.e.\ the individual\rq s utility depends on the sum of payoffs. This means the individual exhibits perfect substitutes preferences for giving, or efficiency-minded preferences: they will prefer to give their entire endowment to the other player when the price of giving is cheap (\(p\) $<$ 1), and they will prefer to keep their entire endowment when the price of giving is expensive (\(p\) $>$ 1). As \(\rho \rightarrow -\infty\), \(U_{s} \rightarrow \min(\alpha\pi_{s}, (1-\alpha)\pi_{o})\), that is, the individual\rq s utility equals the minimum payoff between both players. These preferences are called Rawlsian preferences, or inequality-averse preferences: the individual prefers to split the endowment equally. Lastly, as \(\rho \rightarrow 0\), \(U_{s} \rightarrow A\pi_{s}^{\alpha}\pi_{o}^{1-\alpha}\). In this case, the individual has Cobb-Douglas preferences.\footnote{See \cite{arrow_1961} on how the different preferences are derived from the CES function.}

Maximizing utility subject to the budget constraint ( \(p_{s}\pi_{s} + p_{o}\pi_{o}=m\) ) yielded the CES demand function given by:
 
\vspace{3mm} \(\pi_{s}(p,m)=\frac{[\alpha/(1-\alpha)]^{1/(1-\rho)}}{\rho^{-\rho/(\rho-1)}+[\alpha/(1-\alpha)]^{(1/(1-\rho)}}m\)

\vspace{4mm} \hspace{14.5mm} \(= \frac{A}{p^{r}+A}m\)
 
\vspace{3mm} \noindent where \(r=-\rho / (1-\rho) \) and \(A=[\alpha / (1-\alpha)]^{1/(1-\rho)} \). This generated the following individual-level econometric specification for each participant \textit{i}: 
 
\vspace{3mm} \( \pi^{t}_{s,i} = \frac{A_{i}}{(p^{t}_{i})^{r_{i}} + A_{i}}m^{t}_{i} + \epsilon^{t}_{i}\)
 
\vspace{3mm} \noindent where \textit{i} represents each participant, \textit{t} represents each independent decision-problems in the generalized dictator game, and \( \epsilon^{t}_{i} \) \ is assumed to be distributed normally with mean zero and variance \(\sigma^{2}_{i}\). For each participant, I used the 9 combinations of \(\pi_{s}\), \(p\), and \textit{m} to calculate the estimates  \( \hat{A}_{i} \) and \( \hat{r}_{i} \) using non-linear least squares. From the estimates, I then retrieved each participant\rq s \( \hat{\rho_{i}}\) and \( \hat{\alpha_{i}} \). I used \(\hat{\alpha}\) as the generalized dictator game parameter to indicate participants\rq \ selfishness, and \(\hat{\rho}\) as the second parameter to represent participants\rq \ preferences for efficiency.

Figure A.1 consists of two panels (Panels A and B) that shows the distribution of  \(\hat{\alpha}\) and \(\hat{\rho}\). The parameter estimates varied dramatically across subjects, implying that preferences for giving were very heterogeneous. Panel A displays a high peak of 21\% at \(\hat{\alpha}\)=1: a considerable amount of participants displayed extremely selfish preferences. There is a smaller peak of 12\% at \(\hat{\alpha}\)=0.5, and there are more \(\hat{\alpha}\) observations above 0.5 than below 0.5. These results indicate that participants tended to have more selfish preferences. 

To facilitate presentation of Panel B, participants with very negative \(\hat{\rho}\) values were combined into the leftmost bar. About 7\% of subjects had efficiency-minded preferences for giving (\(\hat{\rho} \approx 1\)): these subjects preferred to give their entire endowment to Player 2 when the price of giving was less than one, and preferred to keep their entire endowment when the price of giving was greater than one. A little over 20\% of participants demonstrated inequality-averse preferences (\(\hat{\rho}\) far below 0): they preferred splitting the endowment equally. Roughly 15\% of subjects possessed Cobb-Douglas preferences (\(\hat{\rho} \approx 0\)). Many subjects also had intermediate values of \(\hat{\rho}\): 24\% had preferences for increasing total payoffs (\(0.1 \leq \hat{\rho} \leq 0.9\)), and almost 20\% had inequality-averse preferences (\(-0.9 \leq \hat{\rho} \leq 0.9\)).

Results from the proposers in the ultimatum game and trust game were represented by their pass rates (the percentage of the endowment passed to the other player). For example, if the proposer sent 6 tickets, the pass rate was 0.6. In the ultimatum game, higher pass rates indicated higher levels of altruism and/or fairness preferences. In the trust game, higher pass rates indicated higher levels of altruism, fairness preferences, and/or trust. 

Similarly, outcomes for the players in the public goods games were represented by their pass rates into the public fund. Larger amounts contributed into the public fund signaled higher levels of cooperation.

Results from the responders in the ultimatum game were represented as the minimum pass rate the responder accepted. Since responders were presented an ascending list of all possible amounts the proposer could choose to send, minimum pass rates were obtained with the switch point where the responders changed from rejecting an offer amount to accepting an offer amount. Lower minimum accepted pass rates indicated lower levels of selfishness and negative reciprocity, whereas higher minimum accepted pass rates indicated higher levels of selfishness and negative reciprocity.

Figure A.2 consists of four panels (Panels C, D, E, and F) which shows the distribution of responses in the ultimatum, trust, and public goods games. Panel C reports that 42\% of proposers in the ultimatum game gave exactly half of their endowment to the responders, and there were more people who gave contributions lower than half the endowment than contributions higher than half the endowment. These results are in line with the typical patterns in previous literature, which found that a majority of offers were in the range of 0.25-0.50. Correspondingly, Panel D shows the distribution of the minimum offers the responders accepted. With the exception of one participant who showed extreme negative reciprocity by only accepting 10 tickets, everyone accepted an amount less than or equal to half of the endowment. 27\% of participants accepted an offer amount of 0 tickets. 

Panel E shows the distribution of proposers\rq \ pass rates in the trust game. Pass rates were scattered all across the range from offering none of their endowment to offering all of their endowment. There are two maxima, both around 22\%, where participants gave either half the endowment or the entire endowment. Other contributions that had more than 10\% are proposers who gave 0.3 or 0.4 of the given endowment. These results are also broadly in line with typical findings that reported average transfers of roughly half of the endowment.

Panel F shows that almost half the participants in the public goods game sent their entire endowment into the public fund. The next most popular choice (20\%) was to send half their endowment to the public pool, and only 4\% of participants sent nothing into the public pool. Again, these findings match usual results in literature.

Lastly, the trust game asked for the responder\rq s return amount given the proposer\rq s 10 possible offer amounts. For each responder, I regressed their return amount on the offer amount, and retrieved the estimated slope. Thus the results from the responders in the trust game were represented by the estimated slope, which measured the participant\rq s reciprocity, or trustworthiness level. Values closer to one represented more reciprocal behavior, and lower levels closer to zero represented more selfish behavior.\footnote{There may be concern that some participant\rq s decisions did not follow a linear trend. After plotting each participant\rq s return amount in response to the offer amount, I found that with the exception of a few participants, each participant\rq s data points followed a linear slope.} I also calculated the responders\rq s average repayment rate: for each possible amount the proposer could have sent, the responder chose to send back a portion of the offer. I took the average of the responder\rq s return rates.

Figure A.3 contains two panels (Panels G and H) that displays the distribution of reciprocity levels and average repayment rate, respectively. Panel G shows that, with the exception of two participants who demonstrated negative reciprocity (perhaps they did not understand the game), reciprocity levels were very heterogenous between 0 and 1. There was a maximum (27\%) at reciprocity levels around 0.5, and a local maximum (14\%) around 0.35. There was also a small local maximum (10\%) at 0 -- these individuals did not show any reciprocity. Panel H shows a strong peak of repayment rates of around 0.45, and participants showed slightly stronger preference in repaying less than half of what was received compared to repaying more than half of what was received. These results are in line with typical patterns found in previous literature, which reported average repayment rates of nearly half of the transfer.

Table A.1 shows the pairwise correlations between the different game outcomes. A majority of the correlations were statistically significant at the 5\% level (13 out of 21). All of the statistically significant negative correlations involved \(\hat{\alpha}\) from the generalized dictator game and responders\rq \ behavior in the ultimatum game. The negative correlations for \(\hat{\alpha}\) reflected that participants who were more selfish (\(\hat{\alpha} \rightarrow 1\)) were more likely to make smaller contributions in the other games, and the negative correlations for the responders in the ultimatum game reflected that participants who accepted smaller contributions were more likely to make larger contributions in the other game decisions. Otherwise, decisions made in the other games had positive correlations with one another. Overall, these results show that participants generally demonstrated consistent behavior in all the games.

\subsection{Self-Reported Measures of Past Pro-Social Behaviors}

Total SRA scores were obtained by summing across each participant\rq s responses for the 10 items in the SRA Scale (``Never'' = 0, ``Once'' = 1, ``More than once'' = 2, ``Often'' = 3, ``Very often'' = 4). A higher SRA score indicated higher pro-social behavior. Figure A.4 Panel I depicts the distribution of total scores. There was a wide variation in the total SRA scores obtained, with scores ranging between 20 and 48. Scores were centered around 33 and the distribution of the scores was symmetric. Panel J displays the distribution of monetary SRA scores, which were obtained by summing across each participant\rq s responses for only the items related to money (items 2, 3, 4, and 7 in Appendix B).  The large majority of the scores fell between 9 and 12.

Table A.2 contains pairwise correlations between the game responses and the total/monetary SRA scores. None of the game results were significantly correlated with total SRA scores, and only the results from the responders in the trust game and players in the public goods game were significantly correlated with monetary SRA scores. Overall, there was a very weak relationship between the social preference game results and self-reported measures. 

\subsection{Donations Behavior}

Figure A.5 Panel K shows the distribution of participants\rq \ cumulative donation amounts (donations above \$100 are aggregated into the rightmost bar). Most of the total donation amounts were below \$20 (about 36\%). There were 30 participants who each donated a total amount greater than \$100. All of these larger donations amounts were between \$100 and \$625, with the exception of two extremely large donations. Panel L displays the natural log of average donation amounts\footnote{Older alumni have had more years to donate, so their total donations were expected to generally be larger than younger alumni, as seen in Panel K. To find the average yearly donation amount, I divided total donations by the number of years they have been solicited. For example, the class of 2018 has been solicited for 1 year, and the class of 2013 has been solicited for 6 years.}. The distribution followed a symmetric shape.

Figure A.6 shows that each class year had different donations behavior. Panel M shows that only about a quarter of the senior class participants donated. In contrast, at least 60\% in each alumni class year donated. Panel N shows each class year\rq s average yearly donations. Each class year consecutively had higher donations, from seniors\rq \ average donation of \$3 per participant to the class of 2013\rq s average yearly donation of \$12 per participant\footnote{Donation amounts above \$350 were excluded when calculating each class year\rq s average donations.}.


\subsection{The External Validity of Social Preference Games}

\subsubsection{Do the Games Explain Donations Behavior?}
I now turn to the question of whether the game decisions explain the field behavior. There were two ways that I measured donations behavior: (1) whether the participant has ever donated or not, and (2) the log of each participant\rq s average yearly donations. I included \(\hat{\alpha}\) and \(\hat{\rho}\) parameters from the generalized dictator game (which I will now refer to as ``\(\alpha\)'' and ``\(\rho\)''), proposers\rq \ pass rates in the ultimatum and trust games (``ultimatum1'' and ``trust1''), responders\rq \ minimum accepted pass rates from the ultimatum game (``ultimatum2''), responders\rq \ reciprocity levels from the trust game (``trust2''), and players\rq \ pass rates in the public goods game (``cooperation'') as the explanatory game variables. I also included total and monetary SRA scores (``SRAtotal'' and ``SRAmoney'') as the explanatory self-reported variables. In addition, I included class year, gender, and college major dummy variables\footnote{Although I normalized each class year\rq s average donations by dividing by the number of years they have been solicited, different class years may have different pro-social tendencies. This is supported by Figure A.6 Panel N where average donations increase in class year. Males and females may have different social preferences, and different college majors are known to have different income levels which may affect their donations behavior.}. In the regression models, the baseline class year was the current senior class (class of 2018), and the baseline gender was male. I grouped college majors into three areas of study: arts and humanities, natural sciences and mathematics, and social sciences. The baseline area of study was social sciences. Appendix B contains the table of variable definitions. 

Table A.3a presents twelve logistic regression models using the binary donations metric as the response variable. The first seven columns present each of the game outcomes on their own, and the eighth column includes all of the game variables. The ninth and tenth columns contain only the total and monetary SRA scores, and the last two columns include all of the game outcomes along with the total and monetary SRA scores, respectively. All of the class year dummy variables were statistically significant at the 1\% level, showing that donation participation increased by class year. None of the games or SRA results were statistically significant on their own. Even when both the games and SRA results were used in the regression model, still none of the variables were statistically significant. 

%I then regressed the same explanatory variables on the amount donated. As shown in Figure 5, most donations amounts were fairly small. I therefore used two-limit tobit maximum likelihood, setting the restriction that 0 $\leq$ donations $\leq$ 50.  Table 4 presents the twelve tobit models, presented in the same format as Table 3. The total SRA score is statistically significant at the 10\% level and the monetary SRA score is statistically significant at the 5\% level. Otherwise, none of the social preference games results are statistically significant.
I then used ordinary least squares (OLS) to regress the same explanatory variables on the log of average yearly donations. Table A.4a reports the twelve linear regression models, presented in the same format as Table A.3a. Most of the class year variables were statistically significant at the 5\% level, and showed that donation amounts generally increased by class year. None of the game outcomes or SRA scores were statistically significant at the 5\% level, although trust2 was statistically significant at the 10\% level.

%It is also useful to look at the R$^{2}$ statistic, which measures the proportions of variance explained by the regressors. For both the logistic and tobit regressions, I calculated each model\rq s McFadden\rq s pseudo-R$^{2}$ values\footnote{
%\(R^{2}_{McFadden} = 1 - \frac{log(L_{c})}{log(L_{null})}\)
%where \(L_{c}\) denotes the maximized likelihood for the current fitted model, and \(L_{null}\) denotes the maximized likelihood the model with no predictors.
%}. The values are shown at the bottom of Table 3. The highest value belongs to the model that includes both the games and monetary SRA score. Interestingly, the pseudo-R$^{2}$ value for the model with all of the game variables is lower than the pseudo-R$^{2}$ values for the models that include only the SRA scores. The pseudo-R$^{2}$ values for the tobit regression models are displayed at the bottom of Table 4. The values are very low, and again, the highest value belongs to the model that includes both the games and monetary SRA score. The pseudo-R$^{2}$ value for the model with all of the game variables is higher than the pseudo-R$^{2}$ value for the model that includes only the total SRA score, but is close to the pseudo-R$^{2}$ value for the model that includes only the monetary SRA score. Altogether, these results tell us that self-reported measures are a better tool than even using multiple social preference games when explaining whether people donate or not, multiple social preference games and money-related self-reported measures perform similarly when explaining how much people donate, but the best tool overall is to use both social preference games and self-reported measures. However, all of the R$^{2}$ values are very low - practically zero - so even the ``best'' model explains little to none of the variability of donations behavior.

It is also useful to look at the R$^{2}$ statistic, which measures the proportion of variance explained by the regressors. For the logistic regressions, I calculated each model\rq s McFadden\rq s pseudo-R$^{2}$ values, displayed at the bottom of Table A.3a\footnote{
\(R^{2}_{McFadden} = 1 - \frac{log(L_{c})}{log(L_{null})}\)
where \(L_{c}\) denotes the maximized likelihood for the current fitted model, and \(L_{null}\) denotes the maximized likelihood the model with no predictors.
}. Each model\rq s pseudo-R$^{2}$ values were around 0.14, which means that there was still about 85\% of the variation in donations behavior left unexplained. The adjusted R$^{2}$ values for the linear regression models are displayed at the bottom of Table A.4a. Each value was roughly around 0.05, so there was still about 95\% of the variation in donations that was unexplained. More interestingly, each model\rq s R$^{2}$ values were similar, whether the model included all of the game variables, only SRA scores, or a combination of the two.

Social preference game results explained only a small portion of the variability in donations behaviors, even when combined with self-reported measures. However, perhaps different combinations of the explanatory variables can better explain the field measure. I performed best subset selection on both the logistic and linear regression models\footnote{Best subset selection identifies all of the possible regression models derived from all of the possible combinations of the candidate predictors, and determines the model that does the best at meeting some well-defined criteria. In this case, the criteria I used is Mallows\rq \ \(C_{p}\)-statistic, which assesses assess fits when models with different numbers of parameters are being compared.  See \cite{mallows_1973}.}. Since best subset selection tests all possible combinations of regressors, I included more variables. I added the responders\rq \ average return outcome from the trust game (``average return''), and replaced SRAtotal and SRAmoney with each of the ten individual SRA items (``SRA1'' - ``SRA10''). 

Table A.5 displays the top five best subset logistic regression models along with the dummy variables. Notably, four SRA items (items 3, 4, 8, and 10) appeared in each model, and items 3 and 4 were statistically significant at the 5\% level. The pseudo-R$^{2}$ values were only slightly larger than the logistic models displayed in Table A.3a.

Table A.6 reports the five best subset linear regression models with the dummy variables. Three game variables (ultimatum1, ultimatum2, and trust2) as well as four SRA items (items 3, 4, 5, and 6) appeared in all of the models. Trust2 and SRA items 3 and 4 were statistically significant at the 5\% level. The adjusted R$^{2}$ statistics were quite larger than in the models in Table A.4a; each model explained almost 17\% of the variability in donations behavior. 

Overall, the R$^{2}$ values in Tables A.3--A.6 showed that a majority of the variation in the field measure was still unexplained. Furthermore, some of the game variables have signs that were not expected. First, we would expect the coefficient on \(\alpha\) to be negative since higher levels of \(\alpha\) indicate more selfish behavior. Ultimatum1, trust1, and cooperation should have had positive relationships with donations since higher pass rates signalled pro-social preferences. Ultimatum2 was expected to have a negative relationship with donations since higher minimum accepted pass rates signalled higher negative reciprocity, and trust2 should have had a positive relationship with donations. However, in Table A.3a, the coefficients for \(\alpha\), ultimatum1, ultimatum2, and cooperation had opposite signs than expected. In Table A.4a, the signs on the coefficients for \(\alpha\), trust1, and trust2 were unexpected. Lastly, some of the game variable coefficients in the best subset regression models (Tables A.5 and A.6) also had unexpected signs. Thus the low R$^{2}$ values and unexpected direction of game coefficients suggest that the external validity of social preference games is poor.


\subsubsection{Do the Games Predict Donations Behavior?}

Perhaps the social preference games and self-reported measures can still be a useful tool to predict donations behavior. That is, perhaps Wesleyan University can still use the social preference game results and/or self-reported measures from recent alumni to anticipate donations from incoming alumni.

A common approach to predictive modeling is to split the data into separate training and testing sets. The training set is used to build and train the model, and once the model is ready, the model is tested on the testing set to determine its accuracy and performance. I used the data from the recent alumni participants (those who graduated in 2013, 2014, 2015, and 2016) to create the training set, and used the data from the most recent alumni (those who graduated in 2017) to create the testing set\footnote{I did not use seniors as the testing set because they have very different donations behavior. Using the most recent alumni class as the testing set is satisfactory since the senior class participants will become eventually become the newest alumni class year. Moreover, it is more interesting to predict their donations behavior since Wesleyan University focuses more on alumni donations than senior donations and, as seen in Figure A.6, there is a significant jump in donations.}. 

Similar to the previous section, I used both the binary and continuous representation of donations behavior. Overall, the models I estimated from the training set included: the logistic/linear predictive models, the best subset logistic/linear predictive models, and the least absolute shrinkage and selection operator (lasso) models\footnote{The lasso is a regression analysis method that performs both variable selection and regularization to enhance prediction accuracy. The lasso estimate is defined by\\ \\
\( \beta^{lasso}=\mathop\mathrm{argmin}\limits_{\beta} \{ \frac{1}{2} \sum_{i=1}^{N} (y_{i} - \beta_{0} - \sum_{j=1}^{p}x_{ij}\beta _{j})^{2}+\lambda\sum_{j=1}^{p}|\beta _{j}|\} \) \\ \\
where \(\lambda\) is a free parameter that minimizes the out of sample error. Certain coefficients are truncated to zero to effectively choose a simpler model. In other words, lasso regression picks out the most important coefficients, i.e.\ those that are most predictive (and have the lowest p-values). See \cite{tibshirani_1996} for a full introduction on the lasso method.}. To determine how well the models performed, I compared each of the model's performances on the testing set. For the logistic models I calculated the mean-squared prediction error (MSPE)\footnote{\(MSPE = E[(g(x_{i}) - \hat{g}(x_{i}))^{2}]\), i.e.\ the expected value of the squared difference between the fitted values implied by the predictive model \(\hat{g}\) and the values of the (unobservable) model g.}, and for the linear models I calculated the root mean square error (RMSE)\footnote{\(RMSE = \sqrt{\frac{\Sigma_{i=1}^{n}{(\hat{y}_{i} - y_{i})^2}}{n}}\), i.e.\ the square root of the average sum of squared residuals.}. Smaller error values indicate better performing predictive models.

Table A.7 reports the logistic predictive model results. Each model was tested on the testing set, and the bottom row reports the corresponding MSPE values. The lowest MSPE belonged to the model that included only the monetary SRA score. Table A.8 reports the top five best subset logistic models, and again, the MSPE values are displayed at the bottom. The model that included \(\alpha\), ultimatum2, and SRA items 3, 4, and 10 had the lowest MSPE. Lastly I used the lasso method, and all of the variable coefficients except for \(\alpha\), ultimatum2, and SRA items 3 and 4 were truncated to zero. Table A.9 shows the lasso penalized logistic model\footnote{Statistical significance tests were based on \cite{lockhart_2014}. See \cite{kyung_2010} and Tibshirani for a full discussion on standard errors for lasso predictions.} and corresponding MSPE value.

The table below reports the best performing logistic model, best subset logic model, and lasso penalized logistic model with respective MSPE values:

\vspace{5mm} \begin{adjustbox}{width=\textwidth}
\begin{tabular}{ c | c | c }
\hline \hline
Logistic model & Best subset logistic model & Lasso penalized logistic model \\ 
\hline
\small donated = SRAmoney & donated = \(\alpha\) + ultimatum2+ SRA3 + SRA4 + SRA10 & donated = \(\alpha\) + ultimatum2 + SRA3 + SRA4 \\
 \hline
 0.2465 & 0.2619 & 0.2518 \\  
 \hline \hline
\end{tabular}
\end{adjustbox}

\vspace{5mm} The logistic model including only SRAmoney as the predictor was the most accurate model for predicting donation status. The MSPE value of roughly 0.25 indicates the model correctly predicts whether people will donate or not 75\% of the time. As seen in Figure A.6, about 60\% of all alumni participants donated: we could randomly guess that the participants in the testing set would donate, and be correct about 60\% of the time. Thus the prediction model increased prediction accuracy above this constant baseline by 15\%.

I repeated the same steps for the linear models. Table A.10 reports the linear predictive models; the lowest RMSE corresponded to the model that included only trust2. Table A.11 reports the top five best subset linear models, where the lowest RMSE belonged to the model that included trust2, ultimatum1, cooperation, and SRA items 4, 5, and 6. Finally, the lasso method truncated most of the variable coefficients to zero, leaving \(\rho\), average return, cooperation, and SRA items 4, 5, and 6. Table A.12 displays the lasso model and RMSE value.

%Table 10 reports the linear regression models, where the lowest RMSE corresponds to the model that includes only trust2. I ran best subset selection, and Table 11 reports the top five models. The lowest RMSE belongs to the model that contains \(\rho\), trust2, ultimatum1, cooperation, and SRA items 1, 4, 5, and 6. Finally, the lasso method truncated most of the variables to zero, leaving \(\rho\), average return, cooperation, and SRA items 4, 5, and 6. Table 12 shows the lasso model and its RMSE value.

 
The table below displays the best performing linear model, best subset linear model, and lasso penalized linear model with corresponding RMSE values:
 
\vspace{5mm} \begin{adjustbox}{width=\textwidth}
\begin{tabular}{ c | c | c }
\hline \hline
Linear model & Best subset linear model & Lasso penalized linear model \\
\hline
log(average yearly donations) = trust2 & log(average yearly donations) = trust2 + ultimatum1 + cooperation + & log(average yearly donations) = \(\rho\) + avgreturn + cooperation + \\
\small &  SRA4 + SRA5 + SRA6 & SRA4 + SRA5 + SRA6 \\
\hline
1.8962 & 1.8967 & 1.8811 \\
\hline \hline
\end{tabular}
\end{adjustbox}


\vspace{5mm} The lasso penalized model was the best-performing model for predicting donation amounts. Since I used the natural log of average donations as the response variable, taking the inverse function allows us to better interpret the prediction error: \(e^{1.8811} = 6.5607\), so the model predicted donations with an error of \$6.56. In order to draw conclusions about the performance of the model, a baseline model was needed for comparison. A reasonable estimate was to use the training set\rq s average donation. Testing the baseline model that included only the average donation amount gave an RMSE of 1.9052: \(e^{1.9052} = 6.7208\), so using the average donation of older alumni predicted the donations of the newest alumni class with an error of \$6.72. 

%Testing the baseline model that includes only the average donation amount gave an RMSE of 2.32389: \(e^{2.32389} = 10.21533\), so using the average donation of older alumni predicted the donations of the newest alumni class with an error of \$10.22. 

Overall, using social preference games and self-reported measures increased the prediction accuracy of whether individuals would donate or not by 15\%. However, when predicting the amount donated, the games and self-reported measures perform similarly as simply using the older alumni\rq s average donation to estimate the most recent alumni\rq s average donation.


\section{Conclusion and Discussion}

The accumulated literature exploring the external validity of social preference games has so far yielded mixed results. My thesis provided a systematic approach to this topic, where I collected decisions in four experimental social preference games along with self-reported pro-social behaviors performed in the past. Most importantly, I compared the lab behavior to a natural field measure that was far-removed from my study.

I ran regression models with different combinations of game outcomes and self-reported measures on donations to examine the explanatory power of the social preference games. I observed low R$^{2}$ values -- there was still a lot of variation in the field measure left unexplained. Moreover, a majority of the game coefficients were not statistically significant, and most coefficients even had unexpected directions. Overall it seems that the experimental games do a poor job of explaining the field behavior. The results are still very interesting, however, because they suggest that social preference games may not generalize field behavior as much as it has been anticipated to. Social preference games are thought to at least generally indicate individuals\rq \ pro-social behavior, but with the results from my research, this does not appear to be the case. That said, there may have been a considerable amount of noise in game behavior due to the experimenter demand effect. Thus there wasn\rq t a baseline R$^{2}$ value to compare to, so perhaps the games actually did do a sufficient job explaining donations behavior.

I then ran various prediction models on a training dataset of older alumni participants, and tested the models on a testing dataset of the newest alumni class. The baseline logistic model was using the proportion of those who donated in the training set to guess the proportion of those who would donate in the testing set. The baseline linear model was using the average donations from participants in the training set to guess the donation amounts from participants in the testing set. The games and self-reported measures did a good job in predicting donations status:  using social preference games and self-reported measures increased the accuracy of predicting whether individuals would donate by 15\% given the baseline model. On the other hand, the games and self-reported measures predicted donation amounts no better than the baseline model.

Most notably, the self-reported measures seemed to perform just as well as the games in both explaining and predicting donations behavior. Tables A.3a and A.4a showed that each model\rq s R$^{2}$ values were very close to one another -- the models that included only the total/monetary SRA score explained donations behavior just as well as the models that included multiple game variables. Tables A.7 and A.10 showed that all of the prediction models (whether they included only self-reported measures or multiple game variables) had very similar MSPE/RMSE values.

It can even be argued that the self-reported measures played a more important role than the games. The same SRA items were consistently included in the best subsets and lasso-penalized models (items 3 and 4 in the logistic models in Tables A.8 and A.9; and items 4, 5, and 6 in the linear models in Tables A.11 and A.12). Moreover, Table A.7 showed that the models that contained only total/monetary SRA scores had error values that were smaller than the models that contained multiple social preference games.

This conclusion has a potential policy implication from a research perspective. Experimental games require significant resources; not only is it expensive to pay participants to attend the lab session and to provide money aligned with the payoffs of the games in order to elicit honest actions, but it also takes a lot of time to program the experimental games. Therefore, since self-reported measures seem to explain and predict the field behavior just as well as social preference games, perhaps eliminating the games in favor of survey questions is a more efficient use of resources.

However, there is a limitation to these results. Table A.3b displays the same explanatory logistic models as Table A.3a, but without the class year dummy variables. Similarly, Table A.4b reports the explanatory linear models without the class year dummy variables. Comparing Table A.3b and Table A.4b to the logistic predictive models and linear predictive models in Table A.7 and Table A.10, respectively, the only difference is that the class of 2017 and 2018 were excluded in the predictive models. The game coefficients \(\alpha\), ultimatum2, and trust2 were quite different between the logistic models, and the game coefficients \(\alpha\), ultimatum2, trust1, and cooperation were different between the linear models. Moreover, the game coefficients were still quite different even when including the class year dummy variables (Tables A.3a and A.4a). These results suggest that the relationship between the game outcomes and donations behavior was significantly different for different class years. Perhaps this is why the games do not seem to explain donations behavior very well, even when controlling for class year effects, and only have modest additional predictive power over the baseline models.

There is an additional limitation with the logistic models. Older class years have had more chances to donate. Given the provided data, however, there wasn\rq t a way to normalize the binary donations measure. Therefore we must be cautious when discussing the explanatory power and prediction performance of the logistic models.

Nevertheless, this is just the beginning of a systematic approach to uncovering the external validity of experimental social preference games. In the future, there are a few things that further research can include. First, both Galizzi and Navarro-Martinez and my study used (recent) university students who self-selected into the experiments. It would be beneficial to use a different participant pool, since such subjects could be inherently different than the general population\footnote{See Levitt and List for a discussion on student participants.}. Further research should also use more field measures. Galizzi and Navarro-Martinez created five field situations; however, their subjects were likely influenced by the experimenter demand effect. On the other hand, my study included only one field measure, but the measure was not influenced by the experimenter demand effect. Therefore further research should incorporate more field measures that can be theoretically mapped to various behavioral constructs, but are also far removed from the study itself. Lastly, using other social preference games and/or exploring repeated games could provide further insight into the topic.

%Finally, this research may potentially spark interest into future studies into the external validity of other behavioral economics topics where lab experiments are also commonly used. For example, using experiments to study risk preferences and time preferences is common, so it would be intriguing to adopt a systemic approach to explore whether in-lab behavior is correlated to field measures.



\newpage

\bibliography{mybibliography}


%\bibliographystyle{apalike}

% \newpage
%\begin{thebibliography}{9}
%
%\bibitem{AndreoniMiller}
%Andreoni, J., and Miller, J.H. (2002).
%\textit{Giving according to GARP: An experimental test of the consistency of preferences for altruism}.
%Econometrica, 70, 737-53.
%
%\bibitem{Baran}
%Baran, N.M., Sapienza, P., and Zingales, L. (2010).
%\textit{Can we infer social preferences from the lab? Evidence from the trust game}.
%NBER Working Paper 15654.
%
%\bibitem{Benz}
%Benz, M., and Meier, S. (2006).
%\textit{Do people behave in experiments as in the field? Evidence from donations}.
%Experimental Economics, 11, 268-81.
%
%\bibitem{Berg}
%Berg, J., Dickhaut, J.W., and McCabe, K.A. (1995).
%\textit{Trust, reciprocity, and social history}.
%Games and Economic Behavior, 90, 166-93.
%
%\bibitem{Burnham}
%Burnham, T., McCabe, K., Smith, V. (2000).
%\textit{Friend-or-foe intentionality priming in an extensive form trust game}.
%Journal of Economic Behavior \& Organization, 43, 57-73.
%
%\bibitem{Carpenter}
%Carpenter, J.P., Verhoogen, E., and Burks, S. (2005).
%\textit{The effect of stakes in distribution experiments}.
%Economics Letters, 86, 393-98.
%
%\bibitem{CharnessRabin}
%Charness, G., and Rabin, M. (2002).
%\textit{Understanding social preferences with simple tests}.
%Quarterly Journal of Economics, 117, 817-69.
%
%\bibitem{Cherry}
%Cherry, T., Fykblom, P., and Shogren, J. (2002).
%\textit{Hardnose the Dictator}.
%American Economic Review, 92(4): 1218-21.
%
%\bibitem{Englmaier}
%Englmaier, F., and Gebhardt, G. (2010)
%\textit{Free-riding in the lab and in the field}.
%CESifo Working Paper No. 3612.
%
%\bibitem{FehrLeibbrandt}
%Fehr, E., and Leibbrandt, A. (2011)
%\textit{A field study on cooperativeness and impatience in the Tragedy of the Commons}.
%Journal of Public Economics, 95, 1144-55.
%
%\bibitem{FehrSchmidt}
%Fehr, E., and Schmidt, K. (1999).
%\textit{A theory of fairness, competition, and cooperation}.
%Quarterly Journal of Economics, 114, 173-68.
%
%\bibitem{Fisman1}
%Fisman, R., Jakiela, P., and Kariv, S. (2014).
%\textit{The distributional preferences of Americans}.
%NBER Working Paper.
%
%\bibitem{Fisman2}
%Fisman, R., Kariv, S., and Markovits, D. (2007).
%\textit{Individual Preferences for Giving}.
%American Economic Review, 97(5): 1858-76.
%
%\bibitem{Franzen}
%Franzen, A., and Pointner, S. (2013)
%\textit{The external validity of giving in the dictator game: A field experiment using the misdirected letter technique}.
%Experimental Economics, 16, 155-69.
%
%\bibitem{Galizzi}
%Galizzi, M., and Navarro-Martinez, D. (2017).
%\textit{On the external validity of social preference games: a systematic lab-field study}.
%Management Science.
%
%\bibitem{Gintis}
%Gintis, H. (2000).
%\textit{Strong reciprocity and human sociality}.
%Journal of Theoretical Biology, 206, 169-79.
%
%\bibitem{Gneezy}
%Gneezy, U., Haruvy, E., and Yafe, H. (2004).
%\textit{The inefficiency of splitting the bill: A lesson in institutional design}.
%The Economic Journal, 114(495), 265-80.
%
%\bibitem{Goeschl}
%Goeschl, T., Kettner, S.E., Lohse, J., and Schwieren, C. (2015)
%\textit{What do we learn from public good games about voluntary climate action? Evidence from an artefactual field experiment}.
%University of Heidelberg, Department of Economics Discussion Paper 595.
%
%\bibitem{GurvenWinking}
%Gurven, M., Winking, J. (2008).
%\textit{Collective action in action: pro-social behavior in and out of the laboratory}.
%American Anthropologist, 110(2), 179-190. 
%
%
%\bibitem{Henrich}
%Henrich, J., et al. (2005).
%\textit{``Economic Man'' in Cross-Cultural Perspective: Ethnography and Experiments from 15 Small-Scale Societies}.
%Behavioral and Brain Sciences. 28(6): 795?815.
%
%\bibitem{Hermann}
%Hermann, B., Thoni, C., and Gachter, S. (2008).
%\textit{Anti-social punishment across societies}.
%Science, 319, 1362-67.
%
%\bibitem{HillGurven}
%Hill, K., and Gurven, M. (2004)
%\textit{Economic experiments to examine fairness and cooperation among the Ache Indians of Paraguay}.
%In J. Henrich, R. Boyd, S. Bowles, C. Camerer, E. Fehr, and H. Gintis (Eds.),
%Foundations of Human Sociality: Economic Experiments and Ethnographic Evidence from Fifteen Small-Scale Societies. Oxford University Press.
%
%\bibitem{Hoffman}
%Hoffman, E., McCabe, K., Shachat, K., and Smith, V. (1994).
%\textit{Preferences, property rights, and anonymity in bargaining games}.
%Games and Economic Behavior, 7(3): 346-80.
%
%\bibitem{Karlan}
%Karlan, D.S. (2005).
%\textit{Using experimental economics to measure social capital and predict financial decisions}.
%American Economic Review, 95, 1688-99.
%
%\bibitem{Levitt}
%Levitt, S., and List, J. (2007)
%\textit{What do laboratory experiments measuring social preferences reveal about the real world?}.
%Journal of Economic Perspectives, 21(2), 153-74.
%
%
%\bibitem{List}
%List, John. 2006.
%\textit{The behavioralist meets the market: measuring social preferences and reputation effects in actual transactions}.
%Journal of Political Economy, 114(51), 1-37.
%
%
%\bibitem{Parco}
%Parco, J., Rapoport, A., and Stein, W. (2002).
%\textit{Effects of financial incentives on the breakdown of mutual trust}.
%Psychological Science.
%
%\bibitem{Rabin}
%Rabin, M. (1993).
%\textit{Incorporating fairness into game theory and economics}.
%The American Economic Review, 83, 1281-1302.
%
%\bibitem{Ross}
%Ross, L., and Ward, A. (1996).
%\textit{Naive realism in everyday life: Implications for social conflict and misunderstanding}.
%Values and Knowledge, 103-35.
%
%\bibitem{Rushton}
%Rushton, J.P., Chrisjohn, R.D., and Fekken, G.C. (1981).
%\textit{The altruistic personality and the self-report altruism scale}.
%Personality and Individual Differences, 2, 293-302.
%
%\bibitem{Slonim}
%Slonim, R., and Roth, A. (1998).
%\textit{Learning in high stakes ultimatum games: An experiment in the Slovak Republic}.
%Econometrica, 66, 569-96.
%
%
%\bibitem{Voors}
%Voors, M., Turley, T., Kontoleon, A., Bulte, E., and List, J.A. (2012).
%\textit{Exploring whether behavior in context-free experiments is predictive of behavior in the field: Evidence from lab and field experiments in rural Sierra Leone}.
%Economic Letters, 114, 308-311.
%
%
%\end{thebibliography}
%

\clearpage
\appendix
\counterwithin{figure}{section}
\doublespacing

\input{appendixa}
\newpage
\input{appendixb}
\newpage
\input{appendixc}
\newpage
\input{appendixd}




\end{document}